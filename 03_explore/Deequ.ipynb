{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Check ด้วย Deequ (Spark + SageMaker Processing)\n",
    "\n",
    "ส่วนนี้ตาม template ของคุณ: ใช้ Spark Processing Job + Deequ เพื่อเช็ค data quality เช่น\n",
    "\n",
    "- completeness (หายไหม)\n",
    "\n",
    "- uniqueness ของ (date, store_id)\n",
    "\n",
    "- non-negative constraints (units_sold, base_price)\n",
    "\n",
    "- discount อยู่ในช่วง 0–1 ฯลฯ\n",
    "\n",
    "*ขั้นตอน:*\n",
    "\n",
    "- สร้างสคริปต์ preprocess-deequ-pyspark.py\n",
    "\n",
    "- รัน Spark Processing Job ด้วย PySparkProcessor พร้อม deequ-1.0.3-rc2.jar\n",
    "\n",
    "**ต้องมีไฟล์ deequ-1.0.3-rc2.jar อยู่ในโฟลเดอร์เดียวกับโน้ตบุ๊ก (หรือ path ที่คุณระบุใน submit_jars)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Region: us-east-1\n",
      "SageMaker default bucket: sagemaker-us-east-1-423623839320\n"
     ]
    }
   ],
   "source": [
    "# 3.x Run Deequ data quality checks with Spark Processing Job\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "from time import gmtime, strftime\n",
    "\n",
    "# --- Session / basic setup ---\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "print(\"Region:\", region)\n",
    "print(\"SageMaker default bucket:\", bucket)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using s3_private_path_csv from previous step: s3://sagemaker-us-east-1-423623839320/retail-demand-forecasting/csv/\n",
      "Deequ output root (S3): s3://sagemaker-us-east-1-423623839320/retail-demand-deequ-2025-12-01-09-17-02/output\n"
     ]
    }
   ],
   "source": [
    "# --- Input data path on S3 (CSV ที่เราเขียนขึ้นไปตอน Ingestion) ---\n",
    "\n",
    "# พยายามโหลดตัวแปรจาก step ก่อนหน้าถ้าเคย %store ไว้\n",
    "# ถ้าไม่มี จะ fallback ไปลายเซ็นมาตรฐาน: s3://<bucket>/retail-demand-forecasting/csv/\n",
    "%store -r s3_private_path_csv\n",
    "\n",
    "if \"s3_private_path_csv\" in globals():\n",
    "    s3_input_data = s3_private_path_csv\n",
    "    print(\"Using s3_private_path_csv from previous step:\", s3_input_data)\n",
    "else:\n",
    "    s3_input_data = f\"s3://{bucket}/retail-demand-forecasting/csv/\"\n",
    "    print(\"s3_private_path_csv not found, fallback to:\", s3_input_data)\n",
    "\n",
    "# --- Output prefix for Deequ results ---\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "output_prefix = f\"retail-demand-deequ-{timestamp_prefix}\"\n",
    "\n",
    "# root path ที่สคริปต์จะใช้ต่อไปสร้างโฟลเดอร์ย่อย:\n",
    "#   dataset-metrics, constraint-checks, success-metrics, constraint-suggestions\n",
    "s3_output_analyze_data = f\"s3://{bucket}/{output_prefix}/output\"\n",
    "\n",
    "print(\"Deequ output root (S3):\", s3_output_analyze_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating processing-job with name spark-retail-demand-analyzer-2025-12-01-09-18-16-429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........12-01 09:19 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--jars', '/opt/ml/processing/input/jars', '/opt/ml/processing/input/code/preprocess-deequ-pyspark.py', 's3_input_data', 's3://sagemaker-us-east-1-423623839320/retail-demand-forecasting/csv/', 's3_output_analyze_data', 's3://sagemaker-us-east-1-423623839320/retail-demand-deequ-2025-12-01-09-17-02/output']\n",
      "12-01 09:19 smspark.cli  INFO     Raw spark options before processing: {'jars': '/opt/ml/processing/input/jars', 'class_': None, 'py_files': None, 'files': None, 'verbose': False}\n",
      "12-01 09:19 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/preprocess-deequ-pyspark.py', 's3_input_data', 's3://sagemaker-us-east-1-423623839320/retail-demand-forecasting/csv/', 's3_output_analyze_data', 's3://sagemaker-us-east-1-423623839320/retail-demand-deequ-2025-12-01-09-17-02/output']\n",
      "12-01 09:19 smspark.cli  INFO     Rendered spark options: {'jars': '/opt/ml/processing/input/jars/deequ-1.0.3-rc2.jar', 'class_': None, 'py_files': None, 'files': None, 'verbose': False}\n",
      "12-01 09:19 smspark.cli  INFO     Initializing processing job.\n",
      "12-01 09:19 smspark-submit INFO     {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.2xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.2xlarge', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0', 'topology': None}\n",
      "12-01 09:19 smspark-submit INFO     {'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:423623839320:processing-job/spark-retail-demand-analyzer-2025-12-01-09-18-16-429', 'ProcessingJobName': 'spark-retail-demand-analyzer-2025-12-01-09-18-16-429', 'AppSpecification': {'ImageUri': '173754725891.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark-processing:2.4-cpu', 'ContainerEntrypoint': ['smspark-submit', '--jars', '/opt/ml/processing/input/jars', '/opt/ml/processing/input/code/preprocess-deequ-pyspark.py'], 'ContainerArguments': ['s3_input_data', 's3://sagemaker-us-east-1-423623839320/retail-demand-forecasting/csv/', 's3_output_analyze_data', 's3://sagemaker-us-east-1-423623839320/retail-demand-deequ-2025-12-01-09-17-02/output']}, 'ProcessingInputs': [{'InputName': 'jars', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/jars', 'S3Uri': 's3://sagemaker-us-east-1-423623839320/spark-retail-demand-analyzer-2025-12-01-09-18-16-429/input/jars', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/code', 'S3Uri': 's3://sagemaker-us-east-1-423623839320/spark-retail-demand-analyzer-2025-12-01-09-18-16-429/input/code/preprocess-deequ-pyspark.py', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.2xlarge', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::423623839320:role/service-role/SageMaker-ExecutionRole-20250705T232334', 'StoppingCondition': {'MaxRuntimeInSeconds': 900}}\n",
      "12-01 09:19 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client --jars /opt/ml/processing/input/jars/deequ-1.0.3-rc2.jar /opt/ml/processing/input/code/preprocess-deequ-pyspark.py s3_input_data s3://sagemaker-us-east-1-423623839320/retail-demand-forecasting/csv/ s3_output_analyze_data s3://sagemaker-us-east-1-423623839320/retail-demand-deequ-2025-12-01-09-17-02/output\n",
      "12-01 09:19 smspark-submit INFO     waiting for hosts\n",
      "12-01 09:19 smspark-submit INFO     starting status server\n",
      "12-01 09:19 smspark-submit INFO     Status server listening on algo-1:5555\n",
      "12-01 09:19 smspark-submit INFO     bootstrapping cluster\n",
      "12-01 09:19 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING\n",
      "12-01 09:19 smspark-submit INFO     copying aws jars\n",
      "Serving on http://algo-1:5555\n",
      "12-01 09:19 smspark-submit INFO     Found hadoop jar hadoop-aws-2.10.0-amzn-0.jar\n",
      "12-01 09:19 smspark-submit INFO     Copying optional jar jets3t-0.9.0.jar from /usr/lib/hadoop/lib to /usr/lib/spark/jars\n",
      "12-01 09:19 smspark-submit INFO     copying cluster config\n",
      "12-01 09:19 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml\n",
      "12-01 09:19 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml\n",
      "12-01 09:19 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\n",
      "12-01 09:19 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf\n",
      "12-01 09:19 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh\n",
      "12-01 09:19 root         INFO     Detected instance type: m5.2xlarge with total memory: 32768M and total cores: 8\n",
      "12-01 09:19 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\n",
      "12-01 09:19 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: \n",
      "<?xml version=\"1.0\"?>\n",
      "<!-- Site specific YARN configuration properties -->\n",
      " <configuration>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.hostname</name>\n",
      "         <value>10.0.73.44</value>\n",
      "         <description>The hostname of the RM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.hostname</name>\n",
      "         <value>algo-1</value>\n",
      "         <description>The hostname of the NM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.webapp.address</name>\n",
      "         <value>algo-1:8042</value>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.vmem-pmem-ratio</name>\n",
      "         <value>5</value>\n",
      "         <description>Ratio between virtual memory to physical memory.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.am.max-attempts</name>\n",
      "         <value>1</value>\n",
      "         <description>The maximum number of application attempts.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.env-whitelist</name>\n",
      "         <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>\n",
      "         <description>Environment variable whitelist</description>\n",
      "     </property>\n",
      " \n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
      "    <value>31784</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-vcores</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
      "    <value>8</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
      "    <value>31784</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
      "    <value>8</value>\n",
      "  </property>\n",
      "</configuration>\n",
      "12-01 09:19 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf\n",
      "12-01 09:19 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: \n",
      "spark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\n",
      "spark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\n",
      "spark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\n",
      "spark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\n",
      "spark.driver.host=10.0.73.44\n",
      "spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2\n",
      "spark.driver.memory 2048m\n",
      "spark.driver.memoryOverhead 204m\n",
      "spark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled\n",
      "spark.executor.memory 26847m\n",
      "spark.executor.memoryOverhead 2684m\n",
      "spark.executor.cores 8\n",
      "spark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=2 -XX:ParallelGCThreads=6 \n",
      "spark.executor.instances 1\n",
      "spark.default.parallelism 16\n",
      "12-01 09:19 root         INFO     Finished Yarn configuration files setup.\n",
      "12-01 09:19 root         INFO     No file at /opt/ml/processing/input/conf/configuration.json exists, skipping user configuration\n",
      "25/12/01 09:19:58 INFO namenode.NameNode: STARTUP_MSG: \n",
      "/************************************************************\n",
      "STARTUP_MSG: Starting NameNode\n",
      "STARTUP_MSG:   host = algo-1/10.0.73.44\n",
      "STARTUP_MSG:   args = [-format, -force]\n",
      "STARTUP_MSG:   version = 2.10.0-amzn-0\n",
      "STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//l\n",
      "og4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar\n",
      "STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\n",
      "STARTUP_MSG:   java = 1.8.0_312\n",
      "************************************************************/\n",
      "25/12/01 09:19:58 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
      "25/12/01 09:19:58 INFO namenode.NameNode: createNameNode [-format, -force]\n",
      "Formatting using clusterid: CID-962a1ce9-863c-4293-adfe-92a719625e96\n",
      "25/12/01 09:19:58 INFO namenode.FSEditLog: Edit logging is async:true\n",
      "25/12/01 09:19:58 INFO namenode.FSNamesystem: KeyProvider: null\n",
      "25/12/01 09:19:58 INFO namenode.FSNamesystem: fsLock is fair: true\n",
      "25/12/01 09:19:58 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
      "25/12/01 09:19:58 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\n",
      "25/12/01 09:19:58 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
      "25/12/01 09:19:58 INFO namenode.FSNamesystem: isPermissionEnabled = true\n",
      "25/12/01 09:19:58 INFO namenode.FSNamesystem: HA Enabled: false\n",
      "25/12/01 09:19:58 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
      "25/12/01 09:19:58 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n",
      "25/12/01 09:19:58 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
      "25/12/01 09:19:58 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
      "25/12/01 09:19:58 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Dec 01 09:19:58\n",
      "25/12/01 09:19:58 INFO util.GSet: Computing capacity for map BlocksMap\n",
      "25/12/01 09:19:58 INFO util.GSet: VM type       = 64-bit\n",
      "25/12/01 09:19:58 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB\n",
      "25/12/01 09:19:58 INFO util.GSet: capacity      = 2^21 = 2097152 entries\n",
      "25/12/01 09:19:58 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false\n",
      "25/12/01 09:19:58 WARN conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS\n",
      "25/12/01 09:19:58 WARN conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\n",
      "25/12/01 09:19:58 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
      "25/12/01 09:19:58 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
      "25/12/01 09:19:58 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
      "25/12/01 09:19:58 INFO blockmanagement.BlockManager: defaultReplication         = 3\n",
      "25/12/01 09:19:58 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
      "25/12/01 09:19:58 INFO blockmanagement.BlockManager: minReplication             = 1\n",
      "25/12/01 09:19:58 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
      "25/12/01 09:19:58 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000\n",
      "25/12/01 09:19:58 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
      "25/12/01 09:19:58 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
      "25/12/01 09:19:58 INFO namenode.FSNamesystem: Append Enabled: true\n",
      "25/12/01 09:19:58 INFO namenode.FSDirectory: GLOBAL serial map: bits=24 maxEntries=16777215\n",
      "25/12/01 09:19:58 INFO util.GSet: Computing capacity for map INodeMap\n",
      "25/12/01 09:19:58 INFO util.GSet: VM type       = 64-bit\n",
      "25/12/01 09:19:58 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB\n",
      "25/12/01 09:19:58 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
      "25/12/01 09:19:58 INFO namenode.FSDirectory: ACLs enabled? false\n",
      "25/12/01 09:19:58 INFO namenode.FSDirectory: XAttrs enabled? true\n",
      "25/12/01 09:19:58 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
      "25/12/01 09:19:58 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false\n",
      "25/12/01 09:19:58 INFO util.GSet: Computing capacity for map cachedBlocks\n",
      "25/12/01 09:19:58 INFO util.GSet: VM type       = 64-bit\n",
      "25/12/01 09:19:58 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB\n",
      "25/12/01 09:19:58 INFO util.GSet: capacity      = 2^18 = 262144 entries\n",
      "25/12/01 09:19:58 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
      "25/12/01 09:19:58 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
      "25/12/01 09:19:58 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
      "25/12/01 09:19:58 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
      "25/12/01 09:19:58 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
      "25/12/01 09:19:58 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
      "25/12/01 09:19:58 INFO util.GSet: VM type       = 64-bit\n",
      "25/12/01 09:19:58 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB\n",
      "25/12/01 09:19:58 INFO util.GSet: capacity      = 2^15 = 32768 entries\n",
      "25/12/01 09:19:58 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1522574362-10.0.73.44-1764580798938\n",
      "25/12/01 09:19:58 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\n",
      "25/12/01 09:19:58 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\n",
      "25/12/01 09:19:59 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 323 bytes saved in 0 seconds .\n",
      "25/12/01 09:19:59 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\n",
      "25/12/01 09:19:59 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid = 0 when meet shutdown.\n",
      "25/12/01 09:19:59 INFO namenode.NameNode: SHUTDOWN_MSG: \n",
      "/************************************************************\n",
      "SHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.73.44\n",
      "************************************************************/\n",
      "12-01 09:19 smspark-submit INFO     waiting for cluster to be up\n",
      "25/12/01 09:19:59 INFO datanode.DataNode: STARTUP_MSG: \n",
      "/************************************************************\n",
      "STARTUP_MSG: Starting DataNode\n",
      "STARTUP_MSG:   host = algo-1/10.0.73.44\n",
      "STARTUP_MSG:   args = []\n",
      "STARTUP_MSG:   version = 2.10.0-amzn-0\n",
      "STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//l\n",
      "og4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar\n",
      "STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\n",
      "STARTUP_MSG:   java = 1.8.0_312\n",
      "************************************************************/\n",
      "25/12/01 09:19:59 INFO nodemanager.NodeManager: STARTUP_MSG: \n",
      "/************************************************************\n",
      "STARTUP_MSG: Starting NodeManager\n",
      "STARTUP_MSG:   host = algo-1/10.0.73.44\n",
      "STARTUP_MSG:   args = []\n",
      "STARTUP_MSG:   version = 2.10.0-amzn-0\n",
      "STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/\n",
      ".//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop/etc/hadoop/nm-config/log4j.properties:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.8.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/netty-all-4.0.23.Final.jar\n",
      "STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\n",
      "STARTUP_MSG:   java = 1.8.0_312\n",
      "************************************************************/\n",
      "25/12/01 09:19:59 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
      "25/12/01 09:19:59 INFO resourcemanager.ResourceManager: STARTUP_MSG: \n",
      "/************************************************************\n",
      "STARTUP_MSG: Starting ResourceManager\n",
      "STARTUP_MSG:   host = algo-1/10.0.73.44\n",
      "STARTUP_MSG:   args = []\n",
      "STARTUP_MSG:   version = 2.10.0-amzn-0\n",
      "STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/\n",
      ".//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop/etc/hadoop/rm-config/log4j.properties:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.8.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/netty-all-4.0.23.Final.jar\n",
      "STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\n",
      "STARTUP_MSG:   java = 1.8.0_312\n",
      "************************************************************/\n",
      "25/12/01 09:19:59 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]\n",
      "25/12/01 09:19:59 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]\n",
      "25/12/01 09:19:59 INFO namenode.NameNode: STARTUP_MSG: \n",
      "/************************************************************\n",
      "STARTUP_MSG: Starting NameNode\n",
      "STARTUP_MSG:   host = algo-1/10.0.73.44\n",
      "STARTUP_MSG:   args = []\n",
      "STARTUP_MSG:   version = 2.10.0-amzn-0\n",
      "STARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//l\n",
      "og4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar\n",
      "STARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\n",
      "STARTUP_MSG:   java = 1.8.0_312\n",
      "************************************************************/\n",
      "25/12/01 09:19:59 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\n",
      "25/12/01 09:20:00 INFO namenode.NameNode: createNameNode []\n",
      "25/12/01 09:20:00 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n",
      "25/12/01 09:20:00 INFO conf.Configuration: found resource core-site.xml at file:/etc/hadoop/conf.empty/core-site.xml\n",
      "25/12/01 09:20:00 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "25/12/01 09:20:00 INFO impl.MetricsSystemImpl: NameNode metrics system started\n",
      "25/12/01 09:20:00 INFO security.Groups: clearing userToGroupsMap cache\n",
      "25/12/01 09:20:00 INFO namenode.NameNode: fs.defaultFS is hdfs://10.0.73.44/\n",
      "25/12/01 09:20:00 INFO conf.Configuration: resource-types.xml not found\n",
      "25/12/01 09:20:00 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "25/12/01 09:20:00 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n",
      "25/12/01 09:20:00 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n",
      "25/12/01 09:20:00 INFO conf.Configuration: found resource yarn-site.xml at file:/etc/hadoop/conf.empty/yarn-site.xml\n",
      "25/12/01 09:20:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher\n",
      "25/12/01 09:20:00 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms\n",
      "25/12/01 09:20:00 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms\n",
      "25/12/01 09:20:00 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms\n",
      "25/12/01 09:20:00 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.\n",
      "25/12/01 09:20:00 INFO util.JvmPauseMonitor: Starting JVM pause monitor\n",
      "25/12/01 09:20:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler\n",
      "25/12/01 09:20:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager\n",
      "25/12/01 09:20:00 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\n",
      "25/12/01 09:20:00 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070\n",
      "25/12/01 09:20:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher\n",
      "25/12/01 09:20:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher\n",
      "25/12/01 09:20:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher\n",
      "25/12/01 09:20:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher\n",
      "25/12/01 09:20:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher\n",
      "25/12/01 09:20:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper\n",
      "25/12/01 09:20:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher\n",
      "25/12/01 09:20:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices\n",
      "25/12/01 09:20:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl\n",
      "25/12/01 09:20:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher\n",
      "25/12/01 09:20:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler\n",
      "25/12/01 09:20:00 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode/\n",
      "25/12/01 09:20:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl\n",
      "25/12/01 09:20:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager\n",
      "25/12/01 09:20:00 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\n",
      "25/12/01 09:20:00 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\n",
      "25/12/01 09:20:00 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n",
      "25/12/01 09:20:00 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n",
      "25/12/01 09:20:00 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined\n",
      "25/12/01 09:20:00 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n",
      "25/12/01 09:20:00 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs\n",
      "25/12/01 09:20:00 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\n",
      "25/12/01 09:20:00 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\n",
      "25/12/01 09:20:00 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\n",
      "25/12/01 09:20:00 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "25/12/01 09:20:00 INFO impl.MetricsSystemImpl: ResourceManager metrics system started\n",
      "25/12/01 09:20:00 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "25/12/01 09:20:00 INFO impl.MetricsSystemImpl: NodeManager metrics system started\n",
      "25/12/01 09:20:00 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.\n",
      "25/12/01 09:20:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\n",
      "25/12/01 09:20:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher\n",
      "25/12/01 09:20:00 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean\n",
      "25/12/01 09:20:00 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.\n",
      "25/12/01 09:20:00 INFO nodemanager.DirectoryCollection: Disk Validator: yarn.nodemanager.disk-validator is loaded.\n",
      "25/12/01 09:20:00 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list\n",
      "25/12/01 09:20:00 INFO nodemanager.DirectoryCollection: Disk Validator: yarn.nodemanager.disk-validator is loaded.\n",
      "25/12/01 09:20:00 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "25/12/01 09:20:00 INFO impl.MetricsSystemImpl: DataNode metrics system started\n",
      "25/12/01 09:20:00 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf.empty/capacity-scheduler.xml\n",
      "25/12/01 09:20:00 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)\n",
      "25/12/01 09:20:00 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1, vCores:1>\n",
      "25/12/01 09:20:00 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:31784, vCores:8>\n",
      "25/12/01 09:20:00 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*\n",
      "25/12/01 09:20:00 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n",
      "25/12/01 09:20:00 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n",
      "25/12/01 09:20:00 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\n",
      "25/12/01 09:20:00 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\n",
      "25/12/01 09:20:00 INFO http.HttpServer2: Jetty bound to port 50070\n",
      "25/12/01 09:20:00 INFO mortbay.log: jetty-6.1.26-emr\n",
      "25/12/01 09:20:00 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=ADMINISTER_QUEUE:*SUBMIT_APP:*, labels=*,\n",
      ", reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\n",
      "25/12/01 09:20:00 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\n",
      "25/12/01 09:20:00 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@4a83a74a\n",
      "25/12/01 09:20:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler\n",
      "25/12/01 09:20:00 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService\n",
      "25/12/01 09:20:00 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n",
      "25/12/01 09:20:00 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n",
      "25/12/01 09:20:00 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined\n",
      "25/12/01 09:20:00 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled\n",
      "25/12/01 09:20:00 INFO localizer.ResourceLocalizationService: per directory file limit = 8192\n",
      "25/12/01 09:20:00 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined\n",
      "25/12/01 09:20:00 INFO capacity.LeafQueue: Initializing default\n",
      "capacity = 1.0 [= (float) configuredCapacity / 100 ]\n",
      "absoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\n",
      "maxCapacity = 1.0 [= configuredMaxCapacity ]\n",
      "absoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\n",
      "userLimit = 100 [= configuredUserLimit ]\n",
      "userLimitFactor = 1.0 [= configuredUserLimitFactor ]\n",
      "maxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]\n",
      "maxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\n",
      "usedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\n",
      "absoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\n",
      "maxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\n",
      "minimumAllocationFactor = 0.9999685 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\n",
      "maximumAllocation = <memory:31784, vCores:8> [= configuredMaxAllocation ]\n",
      "numContainers = 0 [= currentNumContainers ]\n",
      "state = RUNNING [= configuredState ]\n",
      "acls = ADMINISTER_QUEUE:*SUBMIT_APP:* [= configuredAcls ]\n",
      "nodeLocalityDelay = 40\n",
      "rackLocalityAdditionalDelay = -1\n",
      "labels=*,\n",
      "reservationsContinueLooking = true\n",
      "preemptionDisabled = true\n",
      "defaultAppPriorityPerQueue = 0\n",
      "priority = 0\n",
      "maxLifetime = -1 seconds\n",
      "defaultLifetime = -1 seconds\n",
      "25/12/01 09:20:00 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0\n",
      "25/12/01 09:20:00 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\n",
      "25/12/01 09:20:00 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\n",
      "25/12/01 09:20:00 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false\n",
      "25/12/01 09:20:00 INFO capacity.WorkflowPriorityMappingsManager: Initialized workflow priority mappings, override: false\n",
      "25/12/01 09:20:00 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1, vCores:1>>, maximumAllocation=<<memory:31784, vCores:8>>, asynchronousScheduling=false, asyncScheduleInterval=5ms\n",
      "25/12/01 09:20:00 INFO conf.Configuration: dynamic-resources.xml not found\n",
      "25/12/01 09:20:00 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].\n",
      "25/12/01 09:20:00 INFO resourcemanager.ResourceManager: TimelineServicePublisher is not configured\n",
      "25/12/01 09:20:01 INFO localizer.ResourceLocalizationService: Disk Validator: yarn.nodemanager.disk-validator is loaded.\n",
      "25/12/01 09:20:01 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker\n",
      "25/12/01 09:20:01 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@710c2b53\n",
      "25/12/01 09:20:01 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null\n",
      "25/12/01 09:20:01 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true\n",
      "25/12/01 09:20:01 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true\n",
      "25/12/01 09:20:01 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true\n",
      "25/12/01 09:20:01 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\n",
      "25/12/01 09:20:01 WARN monitor.ContainersMonitorImpl: NodeManager configured with 31.0 G physical memory allocated to containers, which is more than 80% of the total physical memory available (30.6 G). Thrashing might happen.\n",
      "25/12/01 09:20:01 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.\n",
      "25/12/01 09:20:01 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\n",
      "25/12/01 09:20:01 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined\n",
      "25/12/01 09:20:01 INFO conf.Configuration: resource-types.xml not found\n",
      "25/12/01 09:20:01 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "25/12/01 09:20:01 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n",
      "25/12/01 09:20:01 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n",
      "25/12/01 09:20:01 INFO conf.Configuration: node-resources.xml not found\n",
      "25/12/01 09:20:01 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.\n",
      "25/12/01 09:20:01 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n",
      "25/12/01 09:20:01 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n",
      "25/12/01 09:20:01 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n",
      "25/12/01 09:20:01 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:31784, vCores:8>\n",
      "25/12/01 09:20:01 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster\n",
      "25/12/01 09:20:01 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs\n",
      "25/12/01 09:20:01 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static\n",
      "25/12/01 09:20:01 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster\n",
      "25/12/01 09:20:01 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\n",
      "25/12/01 09:20:01 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\n",
      "25/12/01 09:20:01 INFO http.HttpServer2: adding path spec: /cluster/*\n",
      "25/12/01 09:20:01 INFO http.HttpServer2: adding path spec: /ws/*\n",
      "25/12/01 09:20:01 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
      "25/12/01 09:20:01 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\n",
      "25/12/01 09:20:01 INFO datanode.DataNode: Configured hostname is algo-1\n",
      "25/12/01 09:20:01 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
      "25/12/01 09:20:01 WARN conf.Configuration: No unit for dfs.datanode.outliers.report.interval(1800000) assuming MILLISECONDS\n",
      "25/12/01 09:20:01 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=31784 virtual-memory=158920 virtual-cores=8\n",
      "25/12/01 09:20:01 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0\n",
      "25/12/01 09:20:01 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:50010\n",
      "25/12/01 09:20:01 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s\n",
      "25/12/01 09:20:01 INFO datanode.DataNode: Number threads for balancing is 50\n",
      "25/12/01 09:20:01 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 2000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\n",
      "25/12/01 09:20:01 INFO ipc.Server: Starting Socket Reader #1 for port 0\n",
      "25/12/01 09:20:01 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\n",
      "25/12/01 09:20:01 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070\n",
      "25/12/01 09:20:01 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\n",
      "25/12/01 09:20:01 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined\n",
      "25/12/01 09:20:01 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n",
      "25/12/01 09:20:01 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\n",
      "25/12/01 09:20:01 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\n",
      "25/12/01 09:20:01 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\n",
      "25/12/01 09:20:01 INFO http.HttpServer2: Jetty bound to port 42339\n",
      "25/12/01 09:20:01 INFO mortbay.log: jetty-6.1.26-emr\n",
      "25/12/01 09:20:01 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server\n",
      "25/12/01 09:20:01 INFO ipc.Server: IPC Server Responder: starting\n",
      "25/12/01 09:20:01 INFO ipc.Server: IPC Server listener on 0: starting\n",
      "25/12/01 09:20:01 INFO webapp.WebApps: Registered webapp guice modules\n",
      "25/12/01 09:20:01 INFO http.HttpServer2: Jetty bound to port 8088\n",
      "25/12/01 09:20:01 INFO mortbay.log: jetty-6.1.26-emr\n",
      "25/12/01 09:20:01 INFO security.NMContainerTokenSecretManager: Updating node address : algo-1:34711\n",
      "25/12/01 09:20:01 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 500 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\n",
      "25/12/01 09:20:01 INFO ipc.Server: Starting Socket Reader #1 for port 8040\n",
      "25/12/01 09:20:01 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server\n",
      "25/12/01 09:20:01 INFO ipc.Server: IPC Server Responder: starting\n",
      "25/12/01 09:20:01 INFO ipc.Server: IPC Server listener on 8040: starting\n",
      "25/12/01 09:20:01 INFO localizer.ResourceLocalizationService: Localizer started on port 8040\n",
      "25/12/01 09:20:01 INFO containermanager.ContainerManagerImpl: ContainerManager started at /10.0.73.44:34711\n",
      "25/12/01 09:20:01 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-1/10.0.73.44:0\n",
      "25/12/01 09:20:01 INFO webapp.WebServer: Instantiating NMWebApp at algo-1:8042\n",
      "25/12/01 09:20:01 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!\n",
      "25/12/01 09:20:01 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!\n",
      "25/12/01 09:20:01 INFO namenode.FSEditLog: Edit logging is async:true\n",
      "25/12/01 09:20:01 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\n",
      "25/12/01 09:20:01 INFO mortbay.log: Extract jar:file:/usr/lib/hadoop/hadoop-yarn-common-2.10.0-amzn-0.jar!/webapps/cluster to work/Jetty_10_0_73_44_8088_cluster____.t37d5q/webapp\n",
      "25/12/01 09:20:01 INFO namenode.FSNamesystem: KeyProvider: null\n",
      "25/12/01 09:20:01 INFO namenode.FSNamesystem: fsLock is fair: true\n",
      "25/12/01 09:20:01 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\n",
      "25/12/01 09:20:01 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\n",
      "25/12/01 09:20:01 INFO namenode.FSNamesystem: supergroup          = supergroup\n",
      "25/12/01 09:20:01 INFO namenode.FSNamesystem: isPermissionEnabled = true\n",
      "25/12/01 09:20:01 INFO namenode.FSNamesystem: HA Enabled: false\n",
      "25/12/01 09:20:01 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\n",
      "25/12/01 09:20:01 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\n",
      "25/12/01 09:20:01 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\n",
      "25/12/01 09:20:01 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\n",
      "25/12/01 09:20:01 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Dec 01 09:20:01\n",
      "25/12/01 09:20:01 INFO util.GSet: Computing capacity for map BlocksMap\n",
      "25/12/01 09:20:01 INFO util.GSet: VM type       = 64-bit\n",
      "25/12/01 09:20:01 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:42339\n",
      "25/12/01 09:20:01 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\n",
      "25/12/01 09:20:01 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB\n",
      "25/12/01 09:20:01 INFO util.GSet: capacity      = 2^21 = 2097152 entries\n",
      "25/12/01 09:20:01 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined\n",
      "25/12/01 09:20:01 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false\n",
      "25/12/01 09:20:01 WARN conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS\n",
      "25/12/01 09:20:01 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\n",
      "25/12/01 09:20:01 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node\n",
      "25/12/01 09:20:01 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\n",
      "25/12/01 09:20:01 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\n",
      "25/12/01 09:20:01 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.AuthenticationWithProxyUserFilter) to context node\n",
      "25/12/01 09:20:01 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.AuthenticationWithProxyUserFilter) to context logs\n",
      "25/12/01 09:20:01 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.AuthenticationWithProxyUserFilter) to context static\n",
      "25/12/01 09:20:01 WARN conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\n",
      "25/12/01 09:20:01 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\n",
      "25/12/01 09:20:01 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\n",
      "25/12/01 09:20:01 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\n",
      "25/12/01 09:20:01 INFO http.HttpServer2: adding path spec: /node/*\n",
      "25/12/01 09:20:01 INFO http.HttpServer2: adding path spec: /ws/*\n",
      "25/12/01 09:20:01 INFO blockmanagement.BlockManager: defaultReplication         = 3\n",
      "25/12/01 09:20:01 INFO blockmanagement.BlockManager: maxReplication             = 512\n",
      "25/12/01 09:20:01 INFO blockmanagement.BlockManager: minReplication             = 1\n",
      "25/12/01 09:20:01 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\n",
      "25/12/01 09:20:01 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000\n",
      "25/12/01 09:20:01 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\n",
      "25/12/01 09:20:01 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\n",
      "25/12/01 09:20:01 INFO namenode.FSNamesystem: Append Enabled: true\n",
      "25/12/01 09:20:01 INFO namenode.FSDirectory: GLOBAL serial map: bits=24 maxEntries=16777215\n",
      "25/12/01 09:20:01 INFO util.GSet: Computing capacity for map INodeMap\n",
      "25/12/01 09:20:01 INFO util.GSet: VM type       = 64-bit\n",
      "25/12/01 09:20:01 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB\n",
      "25/12/01 09:20:01 INFO util.GSet: capacity      = 2^20 = 1048576 entries\n",
      "25/12/01 09:20:01 INFO namenode.FSDirectory: ACLs enabled? false\n",
      "25/12/01 09:20:01 INFO namenode.FSDirectory: XAttrs enabled? true\n",
      "25/12/01 09:20:01 INFO namenode.NameNode: Caching file names occurring more than 10 times\n",
      "25/12/01 09:20:01 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false\n",
      "25/12/01 09:20:01 INFO util.GSet: Computing capacity for map cachedBlocks\n",
      "25/12/01 09:20:01 INFO util.GSet: VM type       = 64-bit\n",
      "25/12/01 09:20:01 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB\n",
      "25/12/01 09:20:01 INFO util.GSet: capacity      = 2^18 = 262144 entries\n",
      "25/12/01 09:20:01 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\n",
      "25/12/01 09:20:01 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\n",
      "25/12/01 09:20:01 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\n",
      "25/12/01 09:20:01 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\n",
      "25/12/01 09:20:01 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\n",
      "25/12/01 09:20:01 INFO util.GSet: Computing capacity for map NameNodeRetryCache\n",
      "25/12/01 09:20:01 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075\n",
      "25/12/01 09:20:01 INFO util.GSet: VM type       = 64-bit\n",
      "25/12/01 09:20:01 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB\n",
      "25/12/01 09:20:01 INFO util.GSet: capacity      = 2^15 = 32768 entries\n",
      "25/12/01 09:20:01 INFO util.JvmPauseMonitor: Starting JVM pause monitor\n",
      "25/12/01 09:20:01 INFO datanode.DataNode: dnUserName = root\n",
      "25/12/01 09:20:01 INFO datanode.DataNode: supergroup = supergroup\n",
      "25/12/01 09:20:01 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/namenode/in_use.lock acquired by nodename 103@algo-1\n",
      "25/12/01 09:20:01 INFO namenode.FileJournalManager: Recovering unfinalized segments in /opt/amazon/hadoop/hdfs/namenode/current\n",
      "25/12/01 09:20:01 INFO namenode.FSImage: No edit log streams selected.\n",
      "25/12/01 09:20:01 INFO namenode.FSImage: Planning to load image: FSImageFile(file=/opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)\n",
      "25/12/01 09:20:02 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\n",
      "25/12/01 09:20:02 INFO ipc.Server: Starting Socket Reader #1 for port 50020\n",
      "25/12/01 09:20:02 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.\n",
      "25/12/01 09:20:02 INFO namenode.FSImageFormatPBINode: Successfully loaded 1 inodes\n",
      "25/12/01 09:20:02 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.\n",
      "25/12/01 09:20:02 INFO namenode.FSImage: Loaded image for txid 0 from /opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000\n",
      "25/12/01 09:20:02 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)\n",
      "25/12/01 09:20:02 INFO namenode.FSEditLog: Starting log segment at 1\n",
      "25/12/01 09:20:02 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\n",
      "25/12/01 09:20:02 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\n",
      "25/12/01 09:20:02 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\n",
      "25/12/01 09:20:02 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:50020\n",
      "25/12/01 09:20:02 INFO datanode.DataNode: Refresh request received for nameservices: null\n",
      "25/12/01 09:20:02 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>\n",
      "25/12/01 09:20:02 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.73.44:8020 starting to offer service\n",
      "25/12/01 09:20:02 INFO ipc.Server: IPC Server listener on 50020: starting\n",
      "25/12/01 09:20:02 INFO ipc.Server: IPC Server Responder: starting\n",
      "Dec 01, 2025 9:20:02 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\n",
      "INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class\n",
      "Dec 01, 2025 9:20:02 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\n",
      "INFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class\n",
      "Dec 01, 2025 9:20:02 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\n",
      "INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\n",
      "Dec 01, 2025 9:20:02 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\n",
      "INFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'\n",
      "25/12/01 09:20:02 INFO namenode.NameCache: initialized with 0 entries 0 lookups\n",
      "25/12/01 09:20:02 INFO namenode.FSNamesystem: Finished loading FSImage in 257 msecs\n",
      "25/12/01 09:20:02 INFO webapp.WebApps: Registered webapp guice modules\n",
      "25/12/01 09:20:02 INFO http.HttpServer2: Jetty bound to port 8042\n",
      "25/12/01 09:20:02 INFO mortbay.log: jetty-6.1.26-emr\n",
      "Dec 01, 2025 9:20:02 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\n",
      "INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\n",
      "25/12/01 09:20:02 INFO mortbay.log: Extract jar:file:/usr/lib/hadoop/hadoop-yarn-common-2.10.0-amzn-0.jar!/webapps/node to work/Jetty_algo.1_8042_node____.afclh/webapp\n",
      "25/12/01 09:20:02 INFO namenode.NameNode: RPC server is binding to algo-1:8020\n",
      "25/12/01 09:20:02 INFO namenode.NameNode: Enable NameNode state context:false\n",
      "25/12/01 09:20:02 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\n",
      "25/12/01 09:20:02 INFO ipc.Server: Starting Socket Reader #1 for port 8020\n",
      "25/12/01 09:20:02 INFO namenode.NameNode: Clients are to use algo-1:8020 to access this namenode/service.\n",
      "25/12/01 09:20:02 INFO namenode.FSNamesystem: Registered FSNamesystemState MBean\n",
      "25/12/01 09:20:02 INFO namenode.LeaseManager: Number of blocks under construction: 0\n",
      "25/12/01 09:20:02 INFO blockmanagement.BlockManager: initializing replication queues\n",
      "25/12/01 09:20:02 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs\n",
      "25/12/01 09:20:02 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes\n",
      "25/12/01 09:20:02 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks\n",
      "25/12/01 09:20:02 INFO blockmanagement.BlockManager: Total number of blocks            = 0\n",
      "25/12/01 09:20:02 INFO blockmanagement.BlockManager: Number of invalid blocks          = 0\n",
      "25/12/01 09:20:02 INFO blockmanagement.BlockManager: Number of under-replicated blocks = 0\n",
      "25/12/01 09:20:02 INFO blockmanagement.BlockManager: Number of  over-replicated blocks = 0\n",
      "25/12/01 09:20:02 INFO blockmanagement.BlockManager: Number of blocks being written    = 0\n",
      "25/12/01 09:20:02 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 9 msec\n",
      "25/12/01 09:20:02 INFO ipc.Server: IPC Server Responder: starting\n",
      "25/12/01 09:20:02 INFO ipc.Server: IPC Server listener on 8020: starting\n",
      "25/12/01 09:20:02 INFO namenode.NameNode: NameNode RPC up at: algo-1/10.0.73.44:8020\n",
      "25/12/01 09:20:02 INFO namenode.FSNamesystem: Starting services required for active state\n",
      "25/12/01 09:20:02 INFO namenode.FSDirectory: Initializing quota with 4 thread(s)\n",
      "25/12/01 09:20:02 INFO namenode.FSDirectory: Quota initialization completed in 6 milliseconds\n",
      "name space=1\n",
      "storage space=0\n",
      "storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0\n",
      "25/12/01 09:20:02 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds\n",
      "Dec 01, 2025 9:20:02 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\n",
      "INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\n",
      "Dec 01, 2025 9:20:02 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\n",
      "INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class\n",
      "Dec 01, 2025 9:20:02 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\n",
      "INFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\n",
      "Dec 01, 2025 9:20:02 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\n",
      "INFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class\n",
      "Dec 01, 2025 9:20:02 AM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\n",
      "INFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'\n",
      "Dec 01, 2025 9:20:02 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\n",
      "INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\n",
      "Dec 01, 2025 9:20:02 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\n",
      "INFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\n",
      "Dec 01, 2025 9:20:02 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\n",
      "INFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\n",
      "25/12/01 09:20:02 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@10.0.73.44:8088\n",
      "25/12/01 09:20:02 INFO webapp.WebApps: Web app cluster started at 8088\n",
      "25/12/01 09:20:02 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 100 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\n",
      "25/12/01 09:20:02 INFO ipc.Server: Starting Socket Reader #1 for port 8033\n",
      "25/12/01 09:20:03 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server\n",
      "25/12/01 09:20:03 INFO ipc.Server: IPC Server Responder: starting\n",
      "25/12/01 09:20:03 INFO ipc.Server: IPC Server listener on 8033: starting\n",
      "25/12/01 09:20:03 INFO resourcemanager.ResourceManager: Transitioning to active state\n",
      "25/12/01 09:20:03 INFO recovery.RMStateStore: Updating AMRMToken\n",
      "25/12/01 09:20:03 INFO security.RMContainerTokenSecretManager: Rolling master-key for container-tokens\n",
      "25/12/01 09:20:03 INFO security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens\n",
      "25/12/01 09:20:03 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\n",
      "25/12/01 09:20:03 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 1\n",
      "25/12/01 09:20:03 INFO recovery.RMStateStore: Storing RMDTMasterKey.\n",
      "25/12/01 09:20:03 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\n",
      "25/12/01 09:20:03 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\n",
      "25/12/01 09:20:03 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 2\n",
      "25/12/01 09:20:03 INFO recovery.RMStateStore: Storing RMDTMasterKey.\n",
      "25/12/01 09:20:03 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler\n",
      "25/12/01 09:20:03 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\n",
      "25/12/01 09:20:03 INFO ipc.Server: Starting Socket Reader #1 for port 8031\n",
      "25/12/01 09:20:03 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server\n",
      "25/12/01 09:20:03 INFO ipc.Server: IPC Server Responder: starting\n",
      "25/12/01 09:20:03 INFO ipc.Server: IPC Server listener on 8031: starting\n",
      "25/12/01 09:20:03 INFO util.JvmPauseMonitor: Starting JVM pause monitor\n",
      "25/12/01 09:20:03 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\n",
      "25/12/01 09:20:03 INFO ipc.Server: Starting Socket Reader #1 for port 8030\n",
      "25/12/01 09:20:03 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server\n",
      "Dec 01, 2025 9:20:03 AM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\n",
      "INFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\n",
      "25/12/01 09:20:03 INFO ipc.Server: IPC Server Responder: starting\n",
      "25/12/01 09:20:03 INFO ipc.Server: IPC Server listener on 8030: starting\n",
      "25/12/01 09:20:03 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\n",
      "25/12/01 09:20:03 INFO ipc.Server: Starting Socket Reader #1 for port 8032\n",
      "25/12/01 09:20:03 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server\n",
      "25/12/01 09:20:03 INFO ipc.Server: IPC Server Responder: starting\n",
      "25/12/01 09:20:03 INFO ipc.Server: IPC Server listener on 8032: starting\n",
      "25/12/01 09:20:03 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@algo-1:8042\n",
      "25/12/01 09:20:03 INFO webapp.WebApps: Web app node started at 8042\n",
      "25/12/01 09:20:03 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-1:34711\n",
      "25/12/01 09:20:03 INFO util.JvmPauseMonitor: Starting JVM pause monitor\n",
      "25/12/01 09:20:03 INFO client.RMProxy: Connecting to ResourceManager at /10.0.73.44:8031\n",
      "25/12/01 09:20:03 INFO resourcemanager.ResourceManager: Transitioned to active state\n",
      "25/12/01 09:20:03 INFO ipc.Client: Retrying connect to server: algo-1/10.0.73.44:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\n",
      "25/12/01 09:20:03 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []\n",
      "25/12/01 09:20:03 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]\n",
      "12-01 09:20 smspark-submit INFO     cluster is up\n",
      "12-01 09:20 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING\n",
      "12-01 09:20 smspark-submit INFO     starting executor logs watcher\n",
      "12-01 09:20 smspark-submit INFO     start log event log publisher\n",
      "Starting executor logs watcher on log_dir: /var/log/yarn\n",
      "12-01 09:20 sagemaker-spark-event-logs-publisher INFO     Spark event log not enabled.\n",
      "12-01 09:20 smspark-submit INFO     Waiting for hosts to bootstrap: ['algo-1']\n",
      "12-01 09:20 smspark-submit INFO     Received host statuses: dict_items([('algo-1', StatusMessage(status='WAITING', timestamp='2025-12-01T09:20:03.416478'))])\n",
      "25/12/01 09:20:03 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1/10.0.73.44:8020\n",
      "25/12/01 09:20:03 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)\n",
      "25/12/01 09:20:03 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 104@algo-1\n",
      "25/12/01 09:20:03 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 465826344. Formatting...\n",
      "25/12/01 09:20:03 INFO common.Storage: Generated new storageID DS-aed0ef10-7760-4811-929e-73dcaca64999 for directory /opt/amazon/hadoop/hdfs/datanode\n",
      "25/12/01 09:20:03 INFO common.Storage: Analyzing storage directories for bpid BP-1522574362-10.0.73.44-1764580798938\n",
      "25/12/01 09:20:03 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-1(cmPort: 34711 httpPort: 8042) registered with capability: <memory:31784, vCores:8>, assigned nodeId algo-1:34711\n",
      "25/12/01 09:20:03 INFO rmnode.RMNodeImpl: algo-1:34711 Node Transitioned from NEW to RUNNING\n",
      "25/12/01 09:20:03 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-1522574362-10.0.73.44-1764580798938\n",
      "25/12/01 09:20:03 INFO common.Storage: Block pool storage directory /opt/amazon/hadoop/hdfs/datanode/current/BP-1522574362-10.0.73.44-1764580798938 is not formatted for BP-1522574362-10.0.73.44-1764580798938. Formatting ...\n",
      "25/12/01 09:20:03 INFO common.Storage: Formatting block pool BP-1522574362-10.0.73.44-1764580798938 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-1522574362-10.0.73.44-1764580798938/current\n",
      "25/12/01 09:20:03 INFO capacity.CapacityScheduler: Added node algo-1:34711 clusterResource: <memory:31784, vCores:8>\n",
      "25/12/01 09:20:03 INFO datanode.DataNode: Setting up storage: nsid=465826344;bpid=BP-1522574362-10.0.73.44-1764580798938;lv=-57;nsInfo=lv=-63;cid=CID-962a1ce9-863c-4293-adfe-92a719625e96;nsid=465826344;c=1764580798938;bpid=BP-1522574362-10.0.73.44-1764580798938;dnuuid=null\n",
      "25/12/01 09:20:03 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id -286749589\n",
      "25/12/01 09:20:03 INFO datanode.DataNode: Generated and persisted new Datanode UUID 1e5a010a-d1e6-4909-aeba-0576cfa57c89\n",
      "25/12/01 09:20:03 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id 1506320473\n",
      "25/12/01 09:20:03 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-1:34711 with total resource of <memory:31784, vCores:8>\n",
      "25/12/01 09:20:03 INFO impl.FsDatasetImpl: Added new volume: DS-aed0ef10-7760-4811-929e-73dcaca64999\n",
      "25/12/01 09:20:03 INFO impl.FsDatasetImpl: Added volume - /opt/amazon/hadoop/hdfs/datanode/current, StorageType: DISK\n",
      "25/12/01 09:20:03 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean\n",
      "25/12/01 09:20:03 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode/current\n",
      "25/12/01 09:20:03 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode/current\n",
      "25/12/01 09:20:03 INFO impl.FsDatasetImpl: Adding block pool BP-1522574362-10.0.73.44-1764580798938\n",
      "25/12/01 09:20:03 INFO impl.FsDatasetImpl: Scanning block pool BP-1522574362-10.0.73.44-1764580798938 on volume /opt/amazon/hadoop/hdfs/datanode/current...\n",
      "25/12/01 09:20:03 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-1522574362-10.0.73.44-1764580798938 on /opt/amazon/hadoop/hdfs/datanode/current: 80ms\n",
      "25/12/01 09:20:03 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-1522574362-10.0.73.44-1764580798938: 82ms\n",
      "25/12/01 09:20:03 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-1522574362-10.0.73.44-1764580798938 on volume /opt/amazon/hadoop/hdfs/datanode/current...\n",
      "25/12/01 09:20:03 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-1522574362-10.0.73.44-1764580798938/current/replicas doesn't exist \n",
      "25/12/01 09:20:03 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-1522574362-10.0.73.44-1764580798938 on volume /opt/amazon/hadoop/hdfs/datanode/current: 1ms\n",
      "25/12/01 09:20:03 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-1522574362-10.0.73.44-1764580798938: 2ms\n",
      "25/12/01 09:20:03 INFO datanode.VolumeScanner: Now scanning bpid BP-1522574362-10.0.73.44-1764580798938 on volume /opt/amazon/hadoop/hdfs/datanode\n",
      "25/12/01 09:20:03 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-aed0ef10-7760-4811-929e-73dcaca64999): finished scanning block pool BP-1522574362-10.0.73.44-1764580798938\n",
      "25/12/01 09:20:03 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 12/1/25 9:34 AM with interval of 21600000ms\n",
      "25/12/01 09:20:03 INFO datanode.DataNode: Block pool BP-1522574362-10.0.73.44-1764580798938 (Datanode Uuid 1e5a010a-d1e6-4909-aeba-0576cfa57c89) service to algo-1/10.0.73.44:8020 beginning handshake with NN\n",
      "25/12/01 09:20:03 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-aed0ef10-7760-4811-929e-73dcaca64999): no suitable block pools found to scan.  Waiting 1814399974 ms.\n",
      "25/12/01 09:20:03 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.0.73.44:50010, datanodeUuid=1e5a010a-d1e6-4909-aeba-0576cfa57c89, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-962a1ce9-863c-4293-adfe-92a719625e96;nsid=465826344;c=1764580798938) storage 1e5a010a-d1e6-4909-aeba-0576cfa57c89\n",
      "25/12/01 09:20:03 INFO net.NetworkTopology: Adding a new node: /default-rack/10.0.73.44:50010\n",
      "25/12/01 09:20:03 INFO blockmanagement.BlockReportLeaseManager: Registered DN 1e5a010a-d1e6-4909-aeba-0576cfa57c89 (10.0.73.44:50010).\n",
      "25/12/01 09:20:03 INFO datanode.DataNode: Block pool Block pool BP-1522574362-10.0.73.44-1764580798938 (Datanode Uuid 1e5a010a-d1e6-4909-aeba-0576cfa57c89) service to algo-1/10.0.73.44:8020 successfully registered with NN\n",
      "25/12/01 09:20:03 INFO datanode.DataNode: For namenode algo-1/10.0.73.44:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\n",
      "25/12/01 09:20:03 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-aed0ef10-7760-4811-929e-73dcaca64999 for DN 10.0.73.44:50010\n",
      "25/12/01 09:20:03 INFO BlockStateChange: BLOCK* processReport 0xb9fc206ac781ace5: Processing first storage report for DS-aed0ef10-7760-4811-929e-73dcaca64999 from datanode 1e5a010a-d1e6-4909-aeba-0576cfa57c89\n",
      "25/12/01 09:20:03 INFO BlockStateChange: BLOCK* processReport 0xb9fc206ac781ace5: from storage DS-aed0ef10-7760-4811-929e-73dcaca64999 node DatanodeRegistration(10.0.73.44:50010, datanodeUuid=1e5a010a-d1e6-4909-aeba-0576cfa57c89, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-962a1ce9-863c-4293-adfe-92a719625e96;nsid=465826344;c=1764580798938), blocks: 0, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0\n",
      "25/12/01 09:20:03 INFO datanode.DataNode: Successfully sent block report 0xb9fc206ac781ace5,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 51 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\n",
      "25/12/01 09:20:03 INFO datanode.DataNode: Got finalize command for block pool BP-1522574362-10.0.73.44-1764580798938\n",
      "WARNING: Running pip install with root privileges is generally not a good idea. Try `python3 -m pip install --user` instead.\n",
      "Collecting pydeequ==0.1.5\n",
      "  Downloading pydeequ-0.1.5-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: pydeequ\n",
      "Successfully installed pydeequ-0.1.5\n",
      "WARNING: Running pip install with root privileges is generally not a good idea. Try `python3 -m pip install --user` instead.\n",
      "Collecting pandas==1.1.4\n",
      "  Downloading pandas-1.1.4-cp37-cp37m-manylinux1_x86_64.whl (9.5 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/site-packages (from pandas==1.1.4) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib64/python3.7/site-packages (from pandas==1.1.4) (1.21.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas==1.1.4) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas==1.1.4) (1.16.0)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.3.4\n",
      "    Uninstalling pandas-1.3.4:\n",
      "      Successfully uninstalled pandas-1.3.4\n",
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "pydeequ 0.1.5 requires pyspark==2.4.7, which is not installed.\n",
      "Successfully installed pandas-1.1.4\n",
      "Input  (Spark): s3a://sagemaker-us-east-1-423623839320/retail-demand-forecasting/csv/\n",
      "Output (Spark): s3a://sagemaker-us-east-1-423623839320/retail-demand-deequ-2025-12-01-09-17-02/output\n",
      "25/12/01 09:20:11 INFO spark.SparkContext: Running Spark version 2.4.6-amzn-0\n",
      "25/12/01 09:20:12 INFO spark.SparkContext: Submitted application: RetailDemand-DataQuality-Deequ\n",
      "25/12/01 09:20:12 INFO spark.SecurityManager: Changing view acls to: root\n",
      "25/12/01 09:20:12 INFO spark.SecurityManager: Changing modify acls to: root\n",
      "25/12/01 09:20:12 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "25/12/01 09:20:12 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "25/12/01 09:20:12 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "25/12/01 09:20:12 INFO util.Utils: Successfully started service 'sparkDriver' on port 33963.\n",
      "25/12/01 09:20:12 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "25/12/01 09:20:12 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "25/12/01 09:20:12 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/12/01 09:20:12 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/12/01 09:20:12 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-6f454f2b-5084-407f-a43e-5aea5fb9ce91\n",
      "25/12/01 09:20:12 INFO memory.MemoryStore: MemoryStore started with capacity 1008.9 MB\n",
      "25/12/01 09:20:12 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "25/12/01 09:20:12 INFO util.log: Logging initialized @8845ms\n",
      "25/12/01 09:20:12 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\n",
      "25/12/01 09:20:12 INFO server.Server: Started @8933ms\n",
      "25/12/01 09:20:12 INFO server.AbstractConnector: Started ServerConnector@180a5b52{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\n",
      "25/12/01 09:20:12 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@51f39a1b{/jobs,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@401b2ca2{/jobs/json,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62426e97{/jobs/job,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@661aa3d{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1f86f86f{/stages,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6e411bde{/stages/json,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3dceefd3{/stages/stage,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@78613ec2{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@321be66a{/stages/pool,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28a4e71e{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@74294ad7{/storage,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54ffdbe{/storage/json,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@688a73cb{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6799f5d3{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@59d86405{/environment,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2380d08d{/environment/json,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18afeb1d{/executors,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30b0b2b6{/executors/json,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a5200e9{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@f73b996{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@68e34c66{/static,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@576c2eda{/,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@78a0b899{/api,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@56d90aea{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@277b5930{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:12 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.73.44:4040\n",
      "25/12/01 09:20:13 INFO client.RMProxy: Connecting to ResourceManager at /10.0.73.44:8032\n",
      "25/12/01 09:20:13 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "25/12/01 09:20:13 INFO resourcemanager.ClientRMService: Allocated new applicationId: 1\n",
      "25/12/01 09:20:13 INFO conf.Configuration: resource-types.xml not found\n",
      "25/12/01 09:20:13 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "25/12/01 09:20:13 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\n",
      "25/12/01 09:20:13 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\n",
      "25/12/01 09:20:13 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (31784 MB per container)\n",
      "25/12/01 09:20:13 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "25/12/01 09:20:13 INFO yarn.Client: Setting up container launch context for our AM\n",
      "25/12/01 09:20:13 INFO yarn.Client: Setting up the launch environment for our AM container\n",
      "25/12/01 09:20:13 INFO yarn.Client: Preparing resources for our AM container\n",
      "25/12/01 09:20:13 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "25/12/01 09:20:19 INFO yarn.Client: Uploading resource file:/tmp/spark-70fd965a-b4a6-46a1-aebb-3aa5a6d77503/__spark_libs__3551978775691304125.zip -> hdfs://10.0.73.44/user/root/.sparkStaging/application_1764580803074_0001/__spark_libs__3551978775691304125.zip\n",
      "25/12/01 09:20:19 INFO hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=10.0.73.44:50010 for /user/root/.sparkStaging/application_1764580803074_0001/__spark_libs__3551978775691304125.zip\n",
      "25/12/01 09:20:19 INFO datanode.DataNode: Receiving BP-1522574362-10.0.73.44-1764580798938:blk_1073741825_1001 src: /10.0.73.44:33580 dest: /10.0.73.44:50010\n",
      "25/12/01 09:20:19 INFO DataNode.clienttrace: src: /10.0.73.44:33580, dest: /10.0.73.44:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_645851372_20, offset: 0, srvID: 1e5a010a-d1e6-4909-aeba-0576cfa57c89, blockid: BP-1522574362-10.0.73.44-1764580798938:blk_1073741825_1001, duration(ns): 261795425\n",
      "25/12/01 09:20:19 INFO datanode.DataNode: PacketResponder: BP-1522574362-10.0.73.44-1764580798938:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating\n",
      "25/12/01 09:20:19 INFO hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=10.0.73.44:50010 for /user/root/.sparkStaging/application_1764580803074_0001/__spark_libs__3551978775691304125.zip\n",
      "25/12/01 09:20:19 INFO datanode.DataNode: Receiving BP-1522574362-10.0.73.44-1764580798938:blk_1073741826_1002 src: /10.0.73.44:33582 dest: /10.0.73.44:50010\n",
      "25/12/01 09:20:19 INFO DataNode.clienttrace: src: /10.0.73.44:33582, dest: /10.0.73.44:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_645851372_20, offset: 0, srvID: 1e5a010a-d1e6-4909-aeba-0576cfa57c89, blockid: BP-1522574362-10.0.73.44-1764580798938:blk_1073741826_1002, duration(ns): 219477767\n",
      "25/12/01 09:20:19 INFO datanode.DataNode: PacketResponder: BP-1522574362-10.0.73.44-1764580798938:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating\n",
      "25/12/01 09:20:19 INFO hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=10.0.73.44:50010 for /user/root/.sparkStaging/application_1764580803074_0001/__spark_libs__3551978775691304125.zip\n",
      "25/12/01 09:20:19 INFO datanode.DataNode: Receiving BP-1522574362-10.0.73.44-1764580798938:blk_1073741827_1003 src: /10.0.73.44:33584 dest: /10.0.73.44:50010\n",
      "25/12/01 09:20:20 INFO DataNode.clienttrace: src: /10.0.73.44:33584, dest: /10.0.73.44:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_645851372_20, offset: 0, srvID: 1e5a010a-d1e6-4909-aeba-0576cfa57c89, blockid: BP-1522574362-10.0.73.44-1764580798938:blk_1073741827_1003, duration(ns): 221092572\n",
      "25/12/01 09:20:20 INFO datanode.DataNode: PacketResponder: BP-1522574362-10.0.73.44-1764580798938:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating\n",
      "25/12/01 09:20:20 INFO hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=10.0.73.44:50010 for /user/root/.sparkStaging/application_1764580803074_0001/__spark_libs__3551978775691304125.zip\n",
      "25/12/01 09:20:20 INFO datanode.DataNode: Receiving BP-1522574362-10.0.73.44-1764580798938:blk_1073741828_1004 src: /10.0.73.44:33600 dest: /10.0.73.44:50010\n",
      "25/12/01 09:20:20 INFO DataNode.clienttrace: src: /10.0.73.44:33600, dest: /10.0.73.44:50010, bytes: 13532460, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_645851372_20, offset: 0, srvID: 1e5a010a-d1e6-4909-aeba-0576cfa57c89, blockid: BP-1522574362-10.0.73.44-1764580798938:blk_1073741828_1004, duration(ns): 17093636\n",
      "25/12/01 09:20:20 INFO datanode.DataNode: PacketResponder: BP-1522574362-10.0.73.44-1764580798938:blk_1073741828_1004, type=LAST_IN_PIPELINE terminating\n",
      "25/12/01 09:20:20 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1764580803074_0001/__spark_libs__3551978775691304125.zip is closed by DFSClient_NONMAPREDUCE_645851372_20\n",
      "25/12/01 09:20:20 INFO yarn.Client: Uploading resource file:/opt/ml/processing/input/jars/deequ-1.0.3-rc2.jar -> hdfs://10.0.73.44/user/root/.sparkStaging/application_1764580803074_0001/deequ-1.0.3-rc2.jar\n",
      "25/12/01 09:20:20 INFO hdfs.StateChange: BLOCK* allocate blk_1073741829_1005, replicas=10.0.73.44:50010 for /user/root/.sparkStaging/application_1764580803074_0001/deequ-1.0.3-rc2.jar\n",
      "25/12/01 09:20:20 INFO datanode.DataNode: Receiving BP-1522574362-10.0.73.44-1764580798938:blk_1073741829_1005 src: /10.0.73.44:33614 dest: /10.0.73.44:50010\n",
      "25/12/01 09:20:20 INFO DataNode.clienttrace: src: /10.0.73.44:33614, dest: /10.0.73.44:50010, bytes: 1714634, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_645851372_20, offset: 0, srvID: 1e5a010a-d1e6-4909-aeba-0576cfa57c89, blockid: BP-1522574362-10.0.73.44-1764580798938:blk_1073741829_1005, duration(ns): 3280018\n",
      "25/12/01 09:20:20 INFO datanode.DataNode: PacketResponder: BP-1522574362-10.0.73.44-1764580798938:blk_1073741829_1005, type=LAST_IN_PIPELINE terminating\n",
      "25/12/01 09:20:20 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1764580803074_0001/deequ-1.0.3-rc2.jar is closed by DFSClient_NONMAPREDUCE_645851372_20\n",
      "25/12/01 09:20:20 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://10.0.73.44/user/root/.sparkStaging/application_1764580803074_0001/pyspark.zip\n",
      "25/12/01 09:20:20 INFO hdfs.StateChange: BLOCK* allocate blk_1073741830_1006, replicas=10.0.73.44:50010 for /user/root/.sparkStaging/application_1764580803074_0001/pyspark.zip\n",
      "25/12/01 09:20:20 INFO datanode.DataNode: Receiving BP-1522574362-10.0.73.44-1764580798938:blk_1073741830_1006 src: /10.0.73.44:33630 dest: /10.0.73.44:50010\n",
      "25/12/01 09:20:20 INFO DataNode.clienttrace: src: /10.0.73.44:33630, dest: /10.0.73.44:50010, bytes: 596339, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_645851372_20, offset: 0, srvID: 1e5a010a-d1e6-4909-aeba-0576cfa57c89, blockid: BP-1522574362-10.0.73.44-1764580798938:blk_1073741830_1006, duration(ns): 1550518\n",
      "25/12/01 09:20:20 INFO datanode.DataNode: PacketResponder: BP-1522574362-10.0.73.44-1764580798938:blk_1073741830_1006, type=LAST_IN_PIPELINE terminating\n",
      "25/12/01 09:20:20 INFO namenode.FSNamesystem: BLOCK* blk_1073741830_1006 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/root/.sparkStaging/application_1764580803074_0001/pyspark.zip\n",
      "25/12/01 09:20:20 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1764580803074_0001/pyspark.zip is closed by DFSClient_NONMAPREDUCE_645851372_20\n",
      "25/12/01 09:20:20 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.7-src.zip -> hdfs://10.0.73.44/user/root/.sparkStaging/application_1764580803074_0001/py4j-0.10.7-src.zip\n",
      "25/12/01 09:20:20 INFO hdfs.StateChange: BLOCK* allocate blk_1073741831_1007, replicas=10.0.73.44:50010 for /user/root/.sparkStaging/application_1764580803074_0001/py4j-0.10.7-src.zip\n",
      "25/12/01 09:20:20 INFO datanode.DataNode: Receiving BP-1522574362-10.0.73.44-1764580798938:blk_1073741831_1007 src: /10.0.73.44:33638 dest: /10.0.73.44:50010\n",
      "25/12/01 09:20:20 INFO DataNode.clienttrace: src: /10.0.73.44:33638, dest: /10.0.73.44:50010, bytes: 42437, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_645851372_20, offset: 0, srvID: 1e5a010a-d1e6-4909-aeba-0576cfa57c89, blockid: BP-1522574362-10.0.73.44-1764580798938:blk_1073741831_1007, duration(ns): 1216162\n",
      "25/12/01 09:20:20 INFO datanode.DataNode: PacketResponder: BP-1522574362-10.0.73.44-1764580798938:blk_1073741831_1007, type=LAST_IN_PIPELINE terminating\n",
      "25/12/01 09:20:20 INFO namenode.FSNamesystem: BLOCK* blk_1073741831_1007 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/root/.sparkStaging/application_1764580803074_0001/py4j-0.10.7-src.zip\n",
      "25/12/01 09:20:21 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1764580803074_0001/py4j-0.10.7-src.zip is closed by DFSClient_NONMAPREDUCE_645851372_20\n",
      "25/12/01 09:20:21 INFO yarn.Client: Uploading resource file:/tmp/spark-70fd965a-b4a6-46a1-aebb-3aa5a6d77503/__spark_conf__7404781821835676673.zip -> hdfs://10.0.73.44/user/root/.sparkStaging/application_1764580803074_0001/__spark_conf__.zip\n",
      "25/12/01 09:20:21 INFO hdfs.StateChange: BLOCK* allocate blk_1073741832_1008, replicas=10.0.73.44:50010 for /user/root/.sparkStaging/application_1764580803074_0001/__spark_conf__.zip\n",
      "25/12/01 09:20:21 INFO datanode.DataNode: Receiving BP-1522574362-10.0.73.44-1764580798938:blk_1073741832_1008 src: /10.0.73.44:33640 dest: /10.0.73.44:50010\n",
      "25/12/01 09:20:21 INFO DataNode.clienttrace: src: /10.0.73.44:33640, dest: /10.0.73.44:50010, bytes: 245511, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_645851372_20, offset: 0, srvID: 1e5a010a-d1e6-4909-aeba-0576cfa57c89, blockid: BP-1522574362-10.0.73.44-1764580798938:blk_1073741832_1008, duration(ns): 1907918\n",
      "25/12/01 09:20:21 INFO datanode.DataNode: PacketResponder: BP-1522574362-10.0.73.44-1764580798938:blk_1073741832_1008, type=LAST_IN_PIPELINE terminating\n",
      "25/12/01 09:20:21 INFO namenode.FSNamesystem: BLOCK* blk_1073741832_1008 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/root/.sparkStaging/application_1764580803074_0001/__spark_conf__.zip\n",
      "25/12/01 09:20:21 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1764580803074_0001/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_645851372_20\n",
      "25/12/01 09:20:21 INFO spark.SecurityManager: Changing view acls to: root\n",
      "25/12/01 09:20:21 INFO spark.SecurityManager: Changing modify acls to: root\n",
      "25/12/01 09:20:21 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "25/12/01 09:20:21 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "25/12/01 09:20:21 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "25/12/01 09:20:22 INFO yarn.Client: Submitting application application_1764580803074_0001 to ResourceManager\n",
      "25/12/01 09:20:22 INFO capacity.CapacityScheduler: Application 'application_1764580803074_0001' is submitted without priority hence considering default queue/cluster priority: 0\n",
      "25/12/01 09:20:22 INFO capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1764580803074_0001\n",
      "25/12/01 09:20:22 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 1]. Use the global max attempts instead.\n",
      "25/12/01 09:20:22 INFO resourcemanager.ClientRMService: Application with id 1 submitted by user root\n",
      "25/12/01 09:20:22 INFO rmapp.RMAppImpl: Storing application with id application_1764580803074_0001\n",
      "25/12/01 09:20:22 INFO resourcemanager.RMAuditLogger: USER=root#011IP=10.0.73.44#011OPERATION=Submit Application Request#011TARGET=ClientRMService#011RESULT=SUCCESS#011APPID=application_1764580803074_0001#011QUEUENAME=default\n",
      "25/12/01 09:20:22 INFO recovery.RMStateStore: Storing info for app: application_1764580803074_0001\n",
      "25/12/01 09:20:22 INFO rmapp.RMAppImpl: application_1764580803074_0001 State change from NEW to NEW_SAVING on event = START\n",
      "25/12/01 09:20:22 INFO rmapp.RMAppImpl: application_1764580803074_0001 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED\n",
      "25/12/01 09:20:22 INFO capacity.ParentQueue: Application added - appId: application_1764580803074_0001 user: root leaf-queue of parent: root #applications: 1\n",
      "25/12/01 09:20:22 INFO capacity.CapacityScheduler: Accepted application application_1764580803074_0001 from user: root, in queue: default\n",
      "25/12/01 09:20:22 INFO rmapp.RMAppImpl: application_1764580803074_0001 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED\n",
      "25/12/01 09:20:22 INFO resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1764580803074_0001_000001\n",
      "25/12/01 09:20:22 INFO attempt.RMAppAttemptImpl: appattempt_1764580803074_0001_000001 State change from NEW to SUBMITTED on event = START\n",
      "25/12/01 09:20:22 INFO capacity.LeafQueue: Application application_1764580803074_0001 from user: root activated in queue: default\n",
      "25/12/01 09:20:22 INFO capacity.LeafQueue: Application added - appId: application_1764580803074_0001 user: root, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1\n",
      "25/12/01 09:20:22 INFO capacity.CapacityScheduler: Added Application Attempt appattempt_1764580803074_0001_000001 to scheduler from user root in queue default\n",
      "25/12/01 09:20:22 INFO attempt.RMAppAttemptImpl: appattempt_1764580803074_0001_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED\n",
      "25/12/01 09:20:22 INFO impl.YarnClientImpl: Submitted application application_1764580803074_0001\n",
      "25/12/01 09:20:22 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1764580803074_0001 and attemptId None\n",
      "25/12/01 09:20:23 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1764580803074_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=\n",
      "25/12/01 09:20:23 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:31784, vCores:8>\n",
      "25/12/01 09:20:23 INFO rmcontainer.RMContainerImpl: container_1764580803074_0001_01_000001 Container Transitioned from NEW to ALLOCATED\n",
      "25/12/01 09:20:23 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1764580803074_0001#011CONTAINERID=container_1764580803074_0001_01_000001#011RESOURCE=<memory:896, vCores:1>#011QUEUENAME=default\n",
      "25/12/01 09:20:23 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:34711 for container : container_1764580803074_0001_01_000001\n",
      "25/12/01 09:20:23 INFO rmcontainer.RMContainerImpl: container_1764580803074_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED\n",
      "25/12/01 09:20:23 INFO security.NMTokenSecretManagerInRM: Clear node set for appattempt_1764580803074_0001_000001\n",
      "25/12/01 09:20:23 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.028190285 absoluteUsedCapacity=0.028190285 used=<memory:896, vCores:1> cluster=<memory:31784, vCores:8>\n",
      "25/12/01 09:20:23 INFO capacity.CapacityScheduler: Allocation proposal accepted\n",
      "25/12/01 09:20:23 INFO attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1764580803074_0001 AttemptId: appattempt_1764580803074_0001_000001 MasterContainer: Container: [ContainerId: container_1764580803074_0001_01_000001, AllocationRequestId: 0, Version: 0, NodeId: algo-1:34711, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.73.44:34711 }, ExecutionType: GUARANTEED, ]\n",
      "25/12/01 09:20:23 INFO attempt.RMAppAttemptImpl: appattempt_1764580803074_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED\n",
      "25/12/01 09:20:23 INFO attempt.RMAppAttemptImpl: appattempt_1764580803074_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED\n",
      "25/12/01 09:20:23 INFO amlauncher.AMLauncher: Launching masterappattempt_1764580803074_0001_000001\n",
      "25/12/01 09:20:23 INFO amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1764580803074_0001_01_000001, AllocationRequestId: 0, Version: 0, NodeId: algo-1:34711, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.73.44:34711 }, ExecutionType: GUARANTEED, ] for AM appattempt_1764580803074_0001_000001\n",
      "25/12/01 09:20:23 INFO security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1764580803074_0001_000001\n",
      "25/12/01 09:20:23 INFO security.AMRMTokenSecretManager: Creating password for appattempt_1764580803074_0001_000001\n",
      "25/12/01 09:20:23 INFO ipc.Server: Auth successful for appattempt_1764580803074_0001_000001 (auth:SIMPLE)\n",
      "25/12/01 09:20:23 INFO containermanager.ContainerManagerImpl: Start request for container_1764580803074_0001_01_000001 by user root\n",
      "25/12/01 09:20:23 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1764580803074_0001\n",
      "25/12/01 09:20:23 INFO application.ApplicationImpl: Application application_1764580803074_0001 transitioned from NEW to INITING\n",
      "25/12/01 09:20:23 INFO application.ApplicationImpl: Adding container_1764580803074_0001_01_000001 to application application_1764580803074_0001\n",
      "25/12/01 09:20:23 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.73.44#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1764580803074_0001#011CONTAINERID=container_1764580803074_0001_01_000001\n",
      "25/12/01 09:20:23 INFO application.ApplicationImpl: Application application_1764580803074_0001 transitioned from INITING to RUNNING\n",
      "25/12/01 09:20:23 INFO container.ContainerImpl: Container container_1764580803074_0001_01_000001 transitioned from NEW to LOCALIZING\n",
      "25/12/01 09:20:23 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1764580803074_0001\n",
      "25/12/01 09:20:23 INFO yarn.Client: Application report for application_1764580803074_0001 (state: ACCEPTED)\n",
      "25/12/01 09:20:23 INFO amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1764580803074_0001_01_000001, AllocationRequestId: 0, Version: 0, NodeId: algo-1:34711, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.0.73.44:34711 }, ExecutionType: GUARANTEED, ] for AM appattempt_1764580803074_0001_000001\n",
      "25/12/01 09:20:23 INFO yarn.Client: \n",
      "#011 client token: N/A\n",
      "#011 diagnostics: [Mon Dec 01 09:20:23 +0000 2025] Scheduler has assigned a container for AM, waiting for AM container to be launched\n",
      "#011 ApplicationMaster host: N/A\n",
      "#011 ApplicationMaster RPC port: -1\n",
      "#011 queue: default\n",
      "#011 start time: 1764580822876\n",
      "#011 final status: UNDEFINED\n",
      "#011 tracking URL: http://algo-1:8088/proxy/application_1764580803074_0001/\n",
      "#011 user: root\n",
      "25/12/01 09:20:23 INFO attempt.RMAppAttemptImpl: appattempt_1764580803074_0001_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED\n",
      "25/12/01 09:20:23 INFO rmapp.RMAppImpl: update the launch time for applicationId: application_1764580803074_0001, attemptId: appattempt_1764580803074_0001_000001launchTime: 1764580823966\n",
      "25/12/01 09:20:23 INFO recovery.RMStateStore: Updating info for app: application_1764580803074_0001\n",
      "25/12/01 09:20:23 INFO localizer.ResourceLocalizationService: Created localizer for container_1764580803074_0001_01_000001\n",
      "25/12/01 09:20:24 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1764580803074_0001_01_000001.tokens\n",
      "25/12/01 09:20:24 INFO nodemanager.DefaultContainerExecutor: Initializing user root\n",
      "25/12/01 09:20:24 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1764580803074_0001_01_000001.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1764580803074_0001/container_1764580803074_0001_01_000001.tokens\n",
      "25/12/01 09:20:24 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1764580803074_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1764580803074_0001\n",
      "25/12/01 09:20:24 INFO localizer.ContainerLocalizer: Disk Validator: yarn.nodemanager.disk-validator is loaded.\n",
      "25/12/01 09:20:24 INFO rmcontainer.RMContainerImpl: container_1764580803074_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING\n",
      "25/12/01 09:20:24 INFO yarn.Client: Application report for application_1764580803074_0001 (state: ACCEPTED)\n",
      "25/12/01 09:20:25 INFO yarn.Client: Application report for application_1764580803074_0001 (state: ACCEPTED)\n",
      "25/12/01 09:20:26 INFO container.ContainerImpl: Container container_1764580803074_0001_01_000001 transitioned from LOCALIZING to SCHEDULED\n",
      "25/12/01 09:20:26 INFO scheduler.ContainerScheduler: Starting container [container_1764580803074_0001_01_000001]\n",
      "25/12/01 09:20:26 INFO container.ContainerImpl: Container container_1764580803074_0001_01_000001 transitioned from SCHEDULED to RUNNING\n",
      "25/12/01 09:20:26 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1764580803074_0001_01_000001\n",
      "25/12/01 09:20:26 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1764580803074_0001/container_1764580803074_0001_01_000001/default_container_executor.sh]\n",
      "Handling create event for file: /var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/prelaunch.out\n",
      "Handling create event for file: /var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/prelaunch.err\n",
      "Handling create event for file: /var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stdout\n",
      "Handling create event for file: /var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr\n",
      "25/12/01 09:20:26 INFO yarn.Client: Application report for application_1764580803074_0001 (state: ACCEPTED)\n",
      "25/12/01 09:20:27 INFO yarn.Client: Application report for application_1764580803074_0001 (state: ACCEPTED)\n",
      "25/12/01 09:20:28 INFO monitor.ContainersMonitorImpl: container_1764580803074_0001_01_000001's ip = 10.0.73.44, and hostname = algo-1\n",
      "25/12/01 09:20:28 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1764580803074_0001_01_000001 since CPU usage is not yet available.\n",
      "25/12/01 09:20:28 INFO ipc.Server: Auth successful for appattempt_1764580803074_0001_000001 (auth:SIMPLE)\n",
      "25/12/01 09:20:28 INFO resourcemanager.DefaultAMSProcessor: AM registration appattempt_1764580803074_0001_000001\n",
      "25/12/01 09:20:28 INFO resourcemanager.RMAuditLogger: USER=root#011IP=10.0.73.44#011OPERATION=Register App Master#011TARGET=ApplicationMasterService#011RESULT=SUCCESS#011APPID=application_1764580803074_0001#011APPATTEMPTID=appattempt_1764580803074_0001_000001\n",
      "25/12/01 09:20:28 INFO attempt.RMAppAttemptImpl: appattempt_1764580803074_0001_000001 State change from LAUNCHED to RUNNING on event = REGISTERED\n",
      "25/12/01 09:20:28 INFO rmapp.RMAppImpl: application_1764580803074_0001 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED\n",
      "25/12/01 09:20:28 INFO yarn.Client: Application report for application_1764580803074_0001 (state: RUNNING)\n",
      "25/12/01 09:20:28 INFO yarn.Client: \n",
      "#011 client token: N/A\n",
      "#011 diagnostics: N/A\n",
      "#011 ApplicationMaster host: 10.0.73.44\n",
      "#011 ApplicationMaster RPC port: -1\n",
      "#011 queue: default\n",
      "#011 start time: 1764580822876\n",
      "#011 final status: UNDEFINED\n",
      "#011 tracking URL: http://algo-1:8088/proxy/application_1764580803074_0001/\n",
      "#011 user: root\n",
      "25/12/01 09:20:28 INFO cluster.YarnClientSchedulerBackend: Application application_1764580803074_0001 has started running.\n",
      "25/12/01 09:20:28 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46687.\n",
      "25/12/01 09:20:28 INFO netty.NettyBlockTransferService: Server created on 10.0.73.44:46687\n",
      "25/12/01 09:20:28 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/12/01 09:20:29 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.73.44, 46687, None)\n",
      "25/12/01 09:20:29 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.73.44:46687 with 1008.9 MB RAM, BlockManagerId(driver, 10.0.73.44, 46687, None)\n",
      "25/12/01 09:20:29 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.73.44, 46687, None)\n",
      "25/12/01 09:20:29 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.73.44, 46687, None)\n",
      "25/12/01 09:20:29 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1764580803074_0001), /proxy/application_1764580803074_0001\n",
      "25/12/01 09:20:29 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\n",
      "25/12/01 09:20:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3936e1dd{/metrics/json,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:29 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n",
      "25/12/01 09:20:29 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "25/12/01 09:20:29 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/lib/spark/spark-warehouse').\n",
      "25/12/01 09:20:29 INFO internal.SharedState: Warehouse path is 'file:/usr/lib/spark/spark-warehouse'.\n",
      "25/12/01 09:20:29 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.\n",
      "25/12/01 09:20:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1c409738{/SQL,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:29 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.\n",
      "25/12/01 09:20:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4abc0591{/SQL/json,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:29 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.\n",
      "25/12/01 09:20:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e390630{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:29 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.\n",
      "25/12/01 09:20:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4097640c{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:29 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.\n",
      "25/12/01 09:20:29 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d76469c{/static/sql,null,AVAILABLE,@Spark}\n",
      "25/12/01 09:20:29 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1764580803074_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=\n",
      "25/12/01 09:20:29 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.028190285 absoluteUsedCapacity=0.028190285 used=<memory:896, vCores:1> cluster=<memory:31784, vCores:8>\n",
      "25/12/01 09:20:29 INFO rmcontainer.RMContainerImpl: container_1764580803074_0001_01_000002 Container Transitioned from NEW to ALLOCATED\n",
      "25/12/01 09:20:29 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1764580803074_0001#011CONTAINERID=container_1764580803074_0001_01_000002#011RESOURCE=<memory:29531, vCores:1>#011QUEUENAME=default\n",
      "25/12/01 09:20:29 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.95730555 absoluteUsedCapacity=0.95730555 used=<memory:30427, vCores:2> cluster=<memory:31784, vCores:8>\n",
      "25/12/01 09:20:29 INFO capacity.CapacityScheduler: Allocation proposal accepted\n",
      "25/12/01 09:20:29 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:34711 for container : container_1764580803074_0001_01_000002\n",
      "25/12/01 09:20:29 INFO rmcontainer.RMContainerImpl: container_1764580803074_0001_01_000002 Container Transitioned from ALLOCATED to ACQUIRED\n",
      "25/12/01 09:20:29 INFO ipc.Server: Auth successful for appattempt_1764580803074_0001_000001 (auth:SIMPLE)\n",
      "25/12/01 09:20:29 INFO containermanager.ContainerManagerImpl: Start request for container_1764580803074_0001_01_000002 by user root\n",
      "25/12/01 09:20:29 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.0.73.44#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1764580803074_0001#011CONTAINERID=container_1764580803074_0001_01_000002\n",
      "25/12/01 09:20:29 INFO application.ApplicationImpl: Adding container_1764580803074_0001_01_000002 to application application_1764580803074_0001\n",
      "25/12/01 09:20:29 INFO container.ContainerImpl: Container container_1764580803074_0001_01_000002 transitioned from NEW to LOCALIZING\n",
      "25/12/01 09:20:29 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1764580803074_0001\n",
      "25/12/01 09:20:29 INFO container.ContainerImpl: Container container_1764580803074_0001_01_000002 transitioned from LOCALIZING to SCHEDULED\n",
      "25/12/01 09:20:29 INFO scheduler.ContainerScheduler: Starting container [container_1764580803074_0001_01_000002]\n",
      "25/12/01 09:20:29 INFO container.ContainerImpl: Container container_1764580803074_0001_01_000002 transitioned from SCHEDULED to RUNNING\n",
      "25/12/01 09:20:29 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1764580803074_0001_01_000002\n",
      "25/12/01 09:20:29 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1764580803074_0001/container_1764580803074_0001_01_000002/default_container_executor.sh]\n",
      "Handling create event for file: /var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/prelaunch.out\n",
      "Handling create event for file: /var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/prelaunch.err\n",
      "Handling create event for file: /var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stdout\n",
      "Handling create event for file: /var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr\n",
      "25/12/01 09:20:30 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr] 25/12/01 09:20:27 INFO util.SignalUtils: Registered signal handler for TERM\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr] 25/12/01 09:20:27 INFO util.SignalUtils: Registered signal handler for HUP\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr] 25/12/01 09:20:27 INFO util.SignalUtils: Registered signal handler for INT\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr] 25/12/01 09:20:27 INFO spark.SecurityManager: Changing view acls to: root\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr] 25/12/01 09:20:27 INFO spark.SecurityManager: Changing modify acls to: root\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr] 25/12/01 09:20:27 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr] 25/12/01 09:20:27 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr] 25/12/01 09:20:27 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr] 25/12/01 09:20:27 INFO yarn.ApplicationMaster: Preparing Local resources\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr] 25/12/01 09:20:28 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1764580803074_0001_000001\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr] 25/12/01 09:20:28 INFO client.RMProxy: Connecting to ResourceManager at /10.0.73.44:8030\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr] 25/12/01 09:20:28 INFO yarn.YarnRMClient: Registering the ApplicationMaster\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr] 25/12/01 09:20:29 INFO client.TransportClientFactory: Successfully created connection to /10.0.73.44:33963 after 81 ms (0 ms spent in bootstraps)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr] 25/12/01 09:20:29 INFO yarn.ApplicationMaster: \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr] ===============================================================================\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr] YARN executor launch context:\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]   env:\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]     CLASSPATH -> /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]     SPARK_YARN_STAGING_DIR -> hdfs://10.0.73.44/user/root/.sparkStaging/application_1764580803074_0001\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]     SPARK_NO_DAEMONIZE -> TRUE\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]     SPARK_USER -> root\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]     SPARK_MASTER_HOST -> 10.0.73.44\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]     SPARK_HOME -> /usr/lib/spark\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]     PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr] \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]   command:\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]     LD_LIBRARY_PATH=\\\"/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:$LD_LIBRARY_PATH\\\" \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       {{JAVA_HOME}}/bin/java \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       -server \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       -Xmx26847m \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       '-verbose:gc' \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       '-XX:OnOutOfMemoryError=kill -9 %p' \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       '-XX:+PrintGCDetails' \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       '-XX:+PrintGCDateStamps' \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       '-XX:+UseParallelGC' \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       '-XX:InitiatingHeapOccupancyPercent=70' \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       '-XX:ConcGCThreads=2' \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       '-XX:ParallelGCThreads=6' \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       -Djava.io.tmpdir={{PWD}}/tmp \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       '-Dspark.driver.port=33963' \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       -Dspark.yarn.app.container.log.dir=<LOG_DIR> \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       org.apache.spark.executor.CoarseGrainedExecutorBackend \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       --driver-url \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       spark://CoarseGrainedScheduler@10.0.73.44:33963 \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       --executor-id \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       <executorId> \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       --hostname \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       <hostname> \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       --cores \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       8 \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       --app-id \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       application_1764580803074_0001 \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       --user-class-path \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       file:$PWD/__app__.jar \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       --user-class-path \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       file:$PWD/deequ-1.0.3-rc2.jar \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       1><LOG_DIR>/stdout \\ \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]       2><LOG_DIR>/stderr\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr] \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]   resources:\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]     __spark_conf__ -> resource { scheme: \"hdfs\" host: \"10.0.73.44\" port: -1 file: \"/user/root/.sparkStaging/application_1764580803074_0001/__spark_conf__.zip\" } size: 245511 timestamp: 1764580821655 type: ARCHIVE visibility: PRIVATE\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]     pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.0.73.44\" port: -1 file: \"/user/root/.sparkStaging/application_1764580803074_0001/pyspark.zip\" } size: 596339 timestamp: 1764580820703 type: FILE visibility: PRIVATE\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]     deequ-1.0.3-rc2.jar -> resource { scheme: \"hdfs\" host: \"10.0.73.44\" port: -1 file: \"/user/root/.sparkStaging/application_1764580803074_0001/deequ-1.0.3-rc2.jar\" } size: 1714634 timestamp: 1764580820280 type: FILE visibility: PRIVATE\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]     __spark_libs__ -> resource { scheme: \"hdfs\" host: \"10.0.73.44\" port: -1 file: \"/user/root/.sparkStaging/application_1764580803074_0001/__spark_libs__3551978775691304125.zip\" } size: 416185644 timestamp: 1764580820166 type: ARCHIVE visibility: PRIVATE\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr]     py4j-0.10.7-src.zip -> resource { scheme: \"hdfs\" host: \"10.0.73.44\" port: -1 file: \"/user/root/.sparkStaging/application_1764580803074_0001/py4j-0.10.7-src.zip\" } size: 42437 timestamp: 1764580821126 type: FILE visibility: PRIVATE\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr] \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr] ===============================================================================\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr] 25/12/01 09:20:29 INFO conf.Configuration: resource-types.xml not found\n",
      "25/12/01 09:20:30 INFO rmcontainer.RMContainerImpl: container_1764580803074_0001_01_000002 Container Transitioned from ACQUIRED to RUNNING\n",
      "25/12/01 09:20:31 INFO Configuration.deprecation: fs.s3a.server-side-encryption-key is deprecated. Instead, use fs.s3a.server-side-encryption.key\n",
      "25/12/01 09:20:31 INFO datasources.InMemoryFileIndex: It took 113 ms to list leaf files for 1 paths.\n",
      "25/12/01 09:20:31 INFO monitor.ContainersMonitorImpl: container_1764580803074_0001_01_000002's ip = 10.0.73.44, and hostname = algo-1\n",
      "25/12/01 09:20:31 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1764580803074_0001_01_000002 since CPU usage is not yet available.\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000001/stderr] 2===== Dataset schema =====\n",
      "root\n",
      " |-- record_id: integer (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- store_id: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- is_weekend: integer (nullable = true)\n",
      " |-- is_holiday: integer (nullable = true)\n",
      " |-- holiday_name: string (nullable = true)\n",
      " |-- max_temp_c: double (nullable = true)\n",
      " |-- rainfall_mm: double (nullable = true)\n",
      " |-- is_hot_day: integer (nullable = true)\n",
      " |-- is_rainy_day: integer (nullable = true)\n",
      " |-- base_price: double (nullable = true)\n",
      " |-- discount_pct: double (nullable = true)\n",
      " |-- is_promo: integer (nullable = true)\n",
      " |-- promo_type: string (nullable = true)\n",
      " |-- final_price: double (nullable = true)\n",
      " |-- units_sold: integer (nullable = true)\n",
      "===== Example rows =====\n",
      "25/12/01 09:20:32 INFO datasources.FileSourceStrategy: Pruning directories with: \n",
      "25/12/01 09:20:32 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "25/12/01 09:20:32 INFO datasources.FileSourceStrategy: Output Data Schema: struct<record_id: int, date: string, store_id: int, day_of_week: string, is_weekend: int ... 15 more fields>\n",
      "25/12/01 09:20:32 INFO execution.FileSourceScanExec: Pushed Filters: \n",
      "25/12/01 09:20:32 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.73.44:45298) with ID 1\n",
      "25/12/01 09:20:32 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:33591 with 13.8 GB RAM, BlockManagerId(1, algo-1, 33591, None)\n",
      "25/12/01 09:20:32 INFO scheduler.AppSchedulingInfo: checking for deactivate of application :application_1764580803074_0001\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:30 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 1234@algo-1\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:30 INFO util.SignalUtils: Registered signal handler for TERM\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:30 INFO util.SignalUtils: Registered signal handler for HUP\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:30 INFO util.SignalUtils: Registered signal handler for INT\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:31 INFO spark.SecurityManager: Changing view acls to: root\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:31 INFO spark.SecurityManager: Changing modify acls to: root\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:31 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:31 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:31 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:31 INFO client.TransportClientFactory: Successfully created connection to /10.0.73.44:33963 after 79 ms (0 ms spent in bootstraps)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:31 INFO spark.SecurityManager: Changing view acls to: root\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:31 INFO spark.SecurityManager: Changing modify acls to: root\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:31 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:31 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:31 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:32 INFO client.TransportClientFactory: Successfully created connection to /10.0.73.44:33963 after 1 ms (0 ms spent in bootstraps)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:32 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1764580803074_0001/blockmgr-f7fe450a-44bf-48ea-b35b-b774db633791\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:32 INFO memory.MemoryStore: MemoryStore started with capacity 13.8 GB\n",
      "25/12/01 09:20:32 INFO codegen.CodeGenerator: Code generated in 313.653079 ms\n",
      "25/12/01 09:20:33 INFO codegen.CodeGenerator: Code generated in 50.887557 ms\n",
      "25/12/01 09:20:33 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.8 KB, free 1008.6 MB)\n",
      "25/12/01 09:20:33 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.6 KB, free 1008.6 MB)\n",
      "25/12/01 09:20:33 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.73.44:46687 (size: 27.6 KB, free: 1008.9 MB)\n",
      "25/12/01 09:20:33 INFO spark.SparkContext: Created broadcast 0 from showString at NativeMethodAccessorImpl.java:0\n",
      "25/12/01 09:20:33 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\n",
      "25/12/01 09:20:33 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,1))\n",
      "25/12/01 09:20:33 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "25/12/01 09:20:33 INFO scheduler.DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/12/01 09:20:33 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/12/01 09:20:33 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "25/12/01 09:20:33 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "25/12/01 09:20:33 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/12/01 09:20:33 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 18.9 KB, free 1008.6 MB)\n",
      "25/12/01 09:20:33 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.1 KB, free 1008.5 MB)\n",
      "25/12/01 09:20:33 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.73.44:46687 (size: 8.1 KB, free: 1008.9 MB)\n",
      "25/12/01 09:20:33 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1203\n",
      "25/12/01 09:20:33 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/12/01 09:20:33 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks\n",
      "25/12/01 09:20:33 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8325 bytes)\n",
      "25/12/01 09:20:34 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:33591 (size: 8.1 KB, free: 13.8 GB)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:32 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.0.73.44:33963\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:32 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:32 INFO executor.Executor: Starting executor ID 1 on host algo-1\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:32 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33591.\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:32 INFO netty.NettyBlockTransferService: Server created on algo-1:33591\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:32 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:32 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(1, algo-1, 33591, None)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:32 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(1, algo-1, 33591, None)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:32 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(1, algo-1, 33591, None)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:33 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 0\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:33 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:34 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:34 INFO client.TransportClientFactory: Successfully created connection to /10.0.73.44:46687 after 2 ms (0 ms spent in bootstraps)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:34 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 8.1 KB, free 13.8 GB)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:34 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 145 ms\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:34 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 18.9 KB, free 13.8 GB)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:34 INFO Configuration.deprecation: fs.s3a.server-side-encryption-key is deprecated. Instead, use fs.s3a.server-side-encryption.key\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:34 INFO executor.CoarseGrainedExecutorBackend: eagerFSInit: Eagerly initialized FileSystem at s3://does/not/exist in 2038 ms\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:34 INFO codegen.CodeGenerator: Code generated in 285.830249 ms\n",
      "25/12/01 09:20:35 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:33591 (size: 27.6 KB, free: 13.8 GB)\n",
      "25/12/01 09:20:35 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1511 ms on algo-1 (executor 1) (1/1)\n",
      "25/12/01 09:20:35 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/12/01 09:20:35 INFO scheduler.DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 1.618 s\n",
      "25/12/01 09:20:35 INFO scheduler.DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 1.673375 s\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:34 INFO datasources.FileScanRDD: TID: 0 - Reading current file: path: s3a://sagemaker-us-east-1-423623839320/retail-demand-f+---------+----------+--------+-----------+----------+----------+------------+----------+-----------+----------+------------+----------+------------+--------+----------+-----------+----------+\n",
      "|record_id|date      |store_id|day_of_week|is_weekend|is_holiday|holiday_name|max_temp_c|rainfall_mm|is_hot_day|is_rainy_day|base_price|discount_pct|is_promo|promo_type|final_price|units_sold|\n",
      "+---------+----------+--------+-----------+----------+----------+------------+----------+-----------+----------+------------+----------+------------+--------+----------+-----------+----------+\n",
      "|1        |2024-01-01|1       |Monday     |0         |1         |New Year    |29.0      |2.4        |0         |0           |12.99     |0.0         |0       |none      |12.99      |102       |\n",
      "|2        |2024-01-02|1       |Tuesday    |0         |0         |None        |27.5      |9.3        |0         |1           |14.85     |0.0         |0       |none      |14.85      |82        |\n",
      "|3        |2024-01-03|1       |Wednesday  |0         |0         |None        |27.1      |5.2        |0         |1           |10.92     |0.0         |0       |none      |10.92      |78        |\n",
      "|4        |2024-01-04|1       |Thursday   |0         |0         |None        |24.2      |0.0        |0         |0           |10.7      |0.0         |0       |none      |10.7       |76        |\n",
      "|5        |2024-01-05|1       |Friday     |0         |0         |None        |26.2      |0.0        |0         |0           |13.93     |0.0         |0       |none      |13.93      |81        |\n",
      "+---------+----------+--------+-----------+----------+----------+------------+----------+-----------+----------+------------+----------+------------+--------+----------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "25/12/01 09:20:35 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.73.44:46687 in memory (size: 8.1 KB, free: 1008.9 MB)\n",
      "25/12/01 09:20:35 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:33591 in memory (size: 8.1 KB, free: 13.8 GB)\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 25\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 7\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 12\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 6\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 21\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 30\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 4\n",
      "25/12/01 09:20:35 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.73.44:46687 in memory (size: 27.6 KB, free: 1008.9 MB)\n",
      "25/12/01 09:20:35 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-1:33591 in memory (size: 27.6 KB, free: 13.8 GB)\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 3\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 22\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 17\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 9\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 1\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 26\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 24\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 11\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 5\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 10\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 19\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 28\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 2\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 15\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 29\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 27\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 18\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 13\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 23\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 8\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 16\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 14\n",
      "25/12/01 09:20:35 INFO spark.ContextCleaner: Cleaned accumulator 20\n",
      "25/12/01 09:20:36 INFO datasources.FileSourceStrategy: Pruning directories with: \n",
      "25/12/01 09:20:36 INFO datasources.FileSourceStrategy: Post-Scan Filters: \n",
      "25/12/01 09:20:36 INFO datasources.FileSourceStrategy: Output Data Schema: struct<record_id: int, date: string, store_id: int, day_of_week: string, max_temp_c: double ... 7 more fields>\n",
      "25/12/01 09:20:36 INFO execution.FileSourceScanExec: Pushed Filters: \n",
      "25/12/01 09:20:36 INFO execution.FileSourceScanExec: Pushed Filters: \n",
      "25/12/01 09:20:36 WARN util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "25/12/01 09:20:36 INFO codegen.CodeGenerator: Code generated in 78.970339 ms\n",
      "25/12/01 09:20:36 INFO codegen.CodeGenerator: Code generated in 6.554481 ms\n",
      "25/12/01 09:20:36 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.8 KB, free 1008.6 MB)\n",
      "25/12/01 09:20:36 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.6 KB, free 1008.6 MB)\n",
      "25/12/01 09:20:36 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.73.44:46687 (size: 27.6 KB, free: 1008.9 MB)\n",
      "25/12/01 09:20:36 INFO spark.SparkContext: Created broadcast 2 from collect at AnalysisRunner.scala:323\n",
      "25/12/01 09:20:36 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\n",
      "25/12/01 09:20:36 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,1))\n",
      "25/12/01 09:20:37 INFO scheduler.DAGScheduler: Registering RDD 7 (collect at AnalysisRunner.scala:323) as input to shuffle 0\n",
      "25/12/01 09:20:37 INFO scheduler.DAGScheduler: Got map stage job 1 (collect at AnalysisRunner.scala:323) with 1 output partitions\n",
      "25/12/01 09:20:37 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 1 (collect at AnalysisRunner.scala:323)\n",
      "25/12/01 09:20:37 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "25/12/01 09:20:37 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "25/12/01 09:20:37 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at collect at AnalysisRunner.scala:323), which has no missing parents\n",
      "25/12/01 09:20:37 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 44.5 KB, free 1008.5 MB)\n",
      "25/12/01 09:20:37 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 18.1 KB, free 1008.5 MB)\n",
      "25/12/01 09:20:37 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.73.44:46687 (size: 18.1 KB, free: 1008.9 MB)\n",
      "25/12/01 09:20:37 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1203\n",
      "25/12/01 09:20:37 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at collect at AnalysisRunner.scala:323) (first 15 tasks are for partitions Vector(0))\n",
      "25/12/01 09:20:37 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks\n",
      "25/12/01 09:20:37 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8314 bytes)\n",
      "25/12/01 09:20:37 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:33591 (size: 18.1 KB, free: 13.8 GB)\n",
      "25/12/01 09:20:37 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:33591 (size: 27.6 KB, free: 13.8 GB)\n",
      "orecasting/csv/retail-demand-forecasting.csv, range: 0-37284, partition values: [empty row], isDataPresent: false\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:35 INFO codegen.CodeGenerator: Code generated in 32.33341 ms\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:35 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:35 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.6 KB, free 13.8 GB)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:35 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 13 ms\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:35 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 398.1 KB, free 13.8 GB)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:35 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 2072 bytes result sent to driver\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:37 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:37 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:37 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:37 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 18.1 KB, free 13.8 GB)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:37 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 8 ms\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:37 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 44.5 KB, free 13.8 GB)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:37 INFO codegen.CodeGenerator: Code generated in 6.461477 ms\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:37 INFO datasources.FileScanRDD: TID: 1 - Reading current file: path: s3a://sagemaker-us-east-1-423623839320/retail-demand-forecasting/csv/retail-demand-forecasting.csv, range: 0-37284, partition values: [empty row], isDataPresent: false\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:37 INFO codegen.CodeGenerator: Code generated in 12.233719 ms\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:37 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:37 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.6 KB, free 13.8 GB)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:37 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 10 ms\n",
      "25/12/01 09:20:38 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 973 ms on algo-1 (executor 1) (1/1)\n",
      "25/12/01 09:20:38 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "25/12/01 09:20:38 INFO scheduler.DAGScheduler: ShuffleMapStage 1 (collect at AnalysisRunner.scala:323) finished in 0.992 s\n",
      "25/12/01 09:20:38 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "25/12/01 09:20:38 INFO scheduler.DAGScheduler: running: Set()\n",
      "25/12/01 09:20:38 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "25/12/01 09:20:38 INFO scheduler.DAGScheduler: failed: Set()\n",
      "25/12/01 09:20:38 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 47.\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 60\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 61\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 75\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 65\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 82\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 72\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 73\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 63\n",
      "25/12/01 09:20:38 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.73.44:46687 in memory (size: 18.1 KB, free: 1008.9 MB)\n",
      "25/12/01 09:20:38 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:33591 in memory (size: 18.1 KB, free: 13.8 GB)\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 77\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 74\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 69\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 62\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 80\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 76\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 78\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 68\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 71\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 67\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 83\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 64\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 79\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 81\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 70\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 66\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 59\n",
      "25/12/01 09:20:38 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:323\n",
      "25/12/01 09:20:38 INFO scheduler.DAGScheduler: Got job 2 (collect at AnalysisRunner.scala:323) with 1 output partitions\n",
      "25/12/01 09:20:38 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (collect at AnalysisRunner.scala:323)\n",
      "25/12/01 09:20:38 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
      "25/12/01 09:20:38 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "25/12/01 09:20:38 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[10] at collect at AnalysisRunner.scala:323), which has no missing parents\n",
      "25/12/01 09:20:38 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 54.7 KB, free 1008.5 MB)\n",
      "25/12/01 09:20:38 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 20.2 KB, free 1008.5 MB)\n",
      "25/12/01 09:20:38 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.73.44:46687 (size: 20.2 KB, free: 1008.9 MB)\n",
      "25/12/01 09:20:38 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1203\n",
      "25/12/01 09:20:38 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[10] at collect at AnalysisRunner.scala:323) (first 15 tasks are for partitions Vector(0))\n",
      "25/12/01 09:20:38 INFO cluster.YarnScheduler: Adding task set 3.0 with 1 tasks\n",
      "25/12/01 09:20:38 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2, algo-1, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\n",
      "25/12/01 09:20:38 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:33591 (size: 20.2 KB, free: 13.8 GB)\n",
      "25/12/01 09:20:38 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.73.44:45298\n",
      "25/12/01 09:20:38 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 330 ms on algo-1 (executor 1) (1/1)\n",
      "25/12/01 09:20:38 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "25/12/01 09:20:38 INFO scheduler.DAGScheduler: ResultStage 3 (collect at AnalysisRunner.scala:323) finished in 0.341 s\n",
      "25/12/01 09:20:38 INFO scheduler.DAGScheduler: Job 2 finished: collect at AnalysisRunner.scala:323, took 0.349142 s\n",
      "25/12/01 09:20:38 INFO codegen.CodeGenerator: Code generated in 24.094874 ms\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:37 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estima===== Analyzer metrics =====\n",
      "25/12/01 09:20:38 INFO codegen.CodeGenerator: Code generated in 7.647873 ms\n",
      "25/12/01 09:20:38 INFO codegen.CodeGenerator: Code generated in 6.275972 ms\n",
      "+-----------+-----------------------+-------------------+---------------------+\n",
      "|entity     |instance               |name               |value                |\n",
      "+-----------+-----------------------+-------------------+---------------------+\n",
      "|Column     |positive_base_price    |Compliance         |1.0                  |\n",
      "|Column     |date                   |Completeness       |1.0                  |\n",
      "|Mutlicolumn|units_sold,base_price  |Correlation        |-0.053894666969926404|\n",
      "|Column     |record_id              |Completeness       |1.0                  |\n",
      "|Column     |max_temp_c             |Mean               |30.4306              |\n",
      "|Column     |store_id               |Completeness       |1.0                  |\n",
      "|Column     |store_id               |ApproxCountDistinct|5.0                  |\n",
      "|Column     |non_negative_units_sold|Compliance         |1.0                  |\n",
      "|Column     |discount_pct           |Mean               |0.05460000000000003  |\n",
      "|Mutlicolumn|units_sold,max_temp_c  |Correlation        |0.30129733302844974  |\n",
      "|Column     |units_sold             |Completeness       |1.0                  |\n",
      "|Column     |units_sold             |Mean               |118.306              |\n",
      "|Dataset    |*                      |Size               |500.0                |\n",
      "|Column     |day_of_week            |ApproxCountDistinct|7.0                  |\n",
      "|Column     |base_price             |Completeness       |1.0                  |\n",
      "|Column     |base_price             |Mean               |12.629919999999995   |\n",
      "|Column     |rainfall_mm            |Mean               |3.4208000000000003   |\n",
      "|Column     |valid_discount_range   |Compliance         |1.0                  |\n",
      "+-----------+-----------------------+-------------------+---------------------+\n",
      "25/12/01 09:20:38 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.73.44:46687 in memory (size: 20.2 KB, free: 1008.9 MB)\n",
      "25/12/01 09:20:38 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:33591 in memory (size: 20.2 KB, free: 13.8 GB)\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 43\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 44\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 108\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 98\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 106\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 53\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 37\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 47\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 49\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 45\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 31\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 57\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 101\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 99\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 39\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 32\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 97\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 100\n",
      "25/12/01 09:20:38 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.73.44:46687 in memory (size: 27.6 KB, free: 1008.9 MB)\n",
      "25/12/01 09:20:38 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on algo-1:33591 in memory (size: 27.6 KB, free: 13.8 GB)\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 96\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 103\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 90\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 51\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 46\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 104\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 93\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 89\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 42\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 56\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 38\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 107\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 34\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 94\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 109\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 86\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 35\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 91\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 55\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 41\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 105\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 102\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 50\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 92\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 84\n",
      "ted size 398.1 KB, free 13.8 GB)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:37 INFO codegen.CodeGenerator: Code generated in 23.307031 ms\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:37 INFO codegen.CodeGenerator: Code generated in 75.575483 ms\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:37 INFO codegen.CodeGenerator: Code generated in 5.514538 ms\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:37 INFO codegen.CodeGenerator: Code generated in 7.654311 ms\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:37 INFO codegen.CodeGenerator: Code generated in 48.316155 ms\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:38 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 1). 2429 bytes result sent to driver\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:38 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 2\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:38 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:38 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:38 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:38 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 20.2 KB, free 13.8 GB)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:38 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 8 ms\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:38 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 54.7 KB, free 13.8 GB)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:38 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:38 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.0.73.44:33963)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:38 INFO spark.MapOutputTrackerWorker: Got the output locations\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:38 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:38 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:38 INFO codegen.CodeGenerator: Code generated in 55.599458 ms\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:38 INFO codegen.CodeGenerator: Code generated in 9.999259 ms\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned shuffle 0\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 40\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 85\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 58\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 95\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 88\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 48\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 87\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 33\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 36\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 52\n",
      "25/12/01 09:20:38 INFO spark.ContextCleaner: Cleaned accumulator 54\n",
      "25/12/01 09:20:39 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "25/12/01 09:20:39 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "25/12/01 09:20:39 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\n",
      "25/12/01 09:20:39 INFO datasources.SQLConfCommitterProvider: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\n",
      "25/12/01 09:20:39 INFO codegen.CodeGenerator: Code generated in 10.335389 ms\n",
      "25/12/01 09:20:39 INFO scheduler.DAGScheduler: Registering RDD 13 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "25/12/01 09:20:39 INFO scheduler.DAGScheduler: Got map stage job 3 (save at NativeMethodAccessorImpl.java:0) with 16 output partitions\n",
      "25/12/01 09:20:39 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 4 (save at NativeMethodAccessorImpl.java:0)\n",
      "25/12/01 09:20:39 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "25/12/01 09:20:39 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "25/12/01 09:20:39 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[13] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/12/01 09:20:39 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 5.4 KB, free 1008.9 MB)\n",
      "25/12/01 09:20:39 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.3 KB, free 1008.9 MB)\n",
      "25/12/01 09:20:39 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.73.44:46687 (size: 3.3 KB, free: 1008.9 MB)\n",
      "25/12/01 09:20:39 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1203\n",
      "25/12/01 09:20:39 INFO scheduler.DAGScheduler: Submitting 16 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[13] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\n",
      "25/12/01 09:20:39 INFO cluster.YarnScheduler: Adding task set 4.0 with 16 tasks\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8116 bytes)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 4, algo-1, executor 1, partition 1, PROCESS_LOCAL, 8100 bytes)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 4.0 (TID 5, algo-1, executor 1, partition 2, PROCESS_LOCAL, 8124 bytes)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 4.0 (TID 6, algo-1, executor 1, partition 3, PROCESS_LOCAL, 8108 bytes)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 4.0 (TID 7, algo-1, executor 1, partition 4, PROCESS_LOCAL, 8100 bytes)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 4.0 (TID 8, algo-1, executor 1, partition 5, PROCESS_LOCAL, 8100 bytes)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 4.0 (TID 9, algo-1, executor 1, partition 6, PROCESS_LOCAL, 8108 bytes)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 4.0 (TID 10, algo-1, executor 1, partition 7, PROCESS_LOCAL, 8205 bytes)\n",
      "25/12/01 09:20:39 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:33591 (size: 3.3 KB, free: 13.8 GB)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 4.0 (TID 11, algo-1, executor 1, partition 8, PROCESS_LOCAL, 8124 bytes)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 4.0 (TID 9) in 37 ms on algo-1 (executor 1) (1/16)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 4.0 (TID 12, algo-1, executor 1, partition 9, PROCESS_LOCAL, 8108 bytes)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 4.0 (TID 13, algo-1, executor 1, partition 10, PROCESS_LOCAL, 8100 bytes)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 4.0 (TID 5) in 40 ms on algo-1 (executor 1) (2/16)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 43 ms on algo-1 (executor 1) (3/16)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 4.0 (TID 14, algo-1, executor 1, partition 11, PROCESS_LOCAL, 8092 bytes)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 4.0 (TID 7) in 41 ms on algo-1 (executor 1) (4/16)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 4.0 (TID 15, algo-1, executor 1, partition 12, PROCESS_LOCAL, 8116 bytes)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 4.0 (TID 6) in 44 ms on algo-1 (executor 1) (5/16)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 4.0 (TID 16, algo-1, executor 1, partition 13, PROCESS_LOCAL, 8108 bytes)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 4) in 46 ms on algo-1 (executor 1) (6/16)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 4.0 (TID 17, algo-1, executor 1, partition 14, PROCESS_LOCAL, 8100 bytes)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 4.0 (TID 18, algo-1, executor 1, partition 15, PROCESS_LOCAL, 8205 bytes)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 4.0 (TID 10) in 47 ms on algo-1 (executor 1) (7/16)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 4.0 (TID 8) in 49 ms on algo-1 (executor 1) (8/16)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 4.0 (TID 14) in 16 ms on algo-1 (executor 1) (9/16)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 4.0 (TID 15) in 13 ms on algo-1 (executor 1) (10/16)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 4.0 (TID 17) in 10 ms on algo-1 (executor 1) (11/16)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 4.0 (TID 13) in 19 ms on algo-1 (executor 1) (12/16)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 4.0 (TID 11) in 24 ms on algo-1 (executor 1) (13/16)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 4.0 (TID 16) in 17 ms on algo-1 (executor 1) (14/16)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 4.0 (TID 18) in 15 ms on algo-1 (executor 1) (15/16)\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 4.0 (TID 12) in 25 ms on algo-1 (executor 1) (16/16)\n",
      "25/12/01 09:20:39 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "25/12/01 09:20:39 INFO scheduler.DAGScheduler: ShuffleMapStage 4 (save at NativeMethodAccessorImpl.java:0) finished in 0.073 s\n",
      "25/12/01 09:20:39 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "25/12/01 09:20:39 INFO scheduler.DAGScheduler: running: Set()\n",
      "25/12/01 09:20:39 INFO scheduler.DAGScheduler: waiting: Set()\n",
      "25/12/01 09:20:39 INFO scheduler.DAGScheduler: failed: Set()\n",
      "25/12/01 09:20:39 INFO spark.SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\n",
      "25/12/01 09:20:39 INFO scheduler.DAGScheduler: Got job 4 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/12/01 09:20:39 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (save at NativeMethodAccessorImpl.java:0)\n",
      "25/12/01 09:20:39 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
      "25/12/01 09:20:39 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "25/12/01 09:20:39 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (ShuffledRowRDD[14] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/12/01 09:20:39 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 167.5 KB, free 1008.7 MB)\n",
      "25/12/01 09:20:39 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 60.3 KB, free 1008.7 MB)\n",
      "25/12/01 09:20:39 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.73.44:46687 (size: 60.3 KB, free: 1008.8 MB)\n",
      "25/12/01 09:20:39 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1203\n",
      "25/12/01 09:20:39 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (ShuffledRowRDD[14] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/12/01 09:20:39 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks\n",
      "25/12/01 09:20:39 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 19, algo-1, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\n",
      "25/12/01 09:20:39 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:33591 (size: 60.3 KB, free: 13.8 GB)\n",
      "25/12/01 09:20:39 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.73.44:45298\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:38 INFO codegen.CodeGenerator: Code generated in 10.785654 ms\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:38 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 2). 3325 bytes result sent to driver\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 3\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Running task 0.0 in stage 4.0 (TID 3)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 5\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Running task 1.0 in stage 4.0 (TID 4)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 6\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 7\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 8\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 9\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Running task 2.0 in stage 4.0 (TID 5)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Running task 3.0 in stage 4.0 (TID 6)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Running task 5.0 in stage 4.0 (TID 8)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Running task 4.0 in stage 4.0 (TID 7)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Running task 6.0 in stage 4.0 (TID 9)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Running task 7.0 in stage 4.0 (TID 10)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.3 KB, free 13.8 GB)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 13 ms\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 5.4 KB, free 13.8 GB)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Finished task 5.0 in stage 4.0 (TID 8). 1384 bytes result sent to driver\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Finished task 0.0 in stage 4.0 (TID 3). 1384 bytes result sent to driver\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Finished task 6.0 in stage 4.0 (TID 9). 1384 bytes result sent to driver\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Finished task 7.0 in stage 4.0 (TID 10). 1384 bytes result sent to driver\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Finished task 3.0 in stage 4.0 (TID 6). 1384 bytes result sent to driver\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Finished task 2.0 in stage 4.0 (TID 5). 1384 bytes result sent to driver\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Finished task 4.0 in stage 4.0 (TID 7). 1384 bytes result sent to driver\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Finished task 1.0 in stage 4.0 (TID 4). 1384 bytes result sent to driver\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 11\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Running task 8.0 in stage 4.0 (TID 11)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 12\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 13\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Running task 9.0 in stage 4.0 (TID 12)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 14\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Running task 11.0 in stage 4.0 (TID 14)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 15\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Running task 12.0 in stage 4.0 (TID 15)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 16\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Running task 13.0 in stage 4.0 (TID 16)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 17\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Running task 14.0 in stage 4.0 (TID 17)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Running task 10.0 in stage 4.0 (TID 13)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Finished task 12.0 in stage 4.0 (TID 15). 1384 bytes result sent to driver\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 18\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Finished task 11.0 in stage 4.0 (TID 14). 1384 bytes result sent to driver\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Finished task 14.0 in stage 4.0 (TID 17). 1384 bytes result sent to driver\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Finished task 10.0 in stage 4.0 (TID 13). 1384 bytes result sent to driver\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Running task 15.0 in stage 4.0 (TID 18)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Finished task 8.0 in stage 4.0 (TID 11). 1384 bytes result sent to driver\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Finished task 9.0 in stage 4.0 (TID 12). 1384 bytes result sent to driver\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Finished task 15.0 in stage 4.0 (TID 18). 1384 bytes result sent to driver\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Finished task 13.0 in stage 4.0 (TID 16). 1384 bytes result sent to driver\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 19\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 19)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO spark.MapOutputTrackerWorker: Updating epoch to 2 and clearing cache\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 6\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 60.3 KB, free 13.8 GB)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO broadcast.TorrentBroadcast: Reading broadcast variable 6 took 6 ms\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 167.5 KB, free 13.8 GB)\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\n",
      "25/12/01 09:20:40 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 19) in 1157 ms on algo-1 (executor 1) (1/1)\n",
      "25/12/01 09:20:40 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "25/12/01 09:20:40 INFO scheduler.DAGScheduler: ResultStage 6 (save at NativeMethodAccessorImpl.java:0) finished in 1.175 s\n",
      "25/12/01 09:20:40 INFO scheduler.DAGScheduler: Job 4 finished: save at NativeMethodAccessorImpl.java:0, took 1.178043 s\n",
      "25/12/01 09:20:41 INFO datasources.FileFormatWriter: Write Job 7c558ffc-8a5b-4776-ace6-f4a7a4e4177e committed.\n",
      "25/12/01 09:20:41 INFO datasources.FileFormatWriter: Finished processing stats for write job 7c558ffc-8a5b-4776-ace6-f4a7a4e4177e.\n",
      "[/var/log/yarn/userlogs/application_1764580803074_0001/container_1764580803074_0001_01_000002/stderr] 25/12/01 09:20:39 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endPython Callback server started!\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/ml/processing/input/code/preprocess-deequ-pyspark.py\", line 235, in <module>\n",
      "    main()\n",
      "  File \"/opt/ml/processing/input/code/preprocess-deequ-pyspark.py\", line 174, in main\n",
      "    .isContainedIn(\"is_weekend\", [0, 1])\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pydeequ/checks.py\", line 747, in isContainedIn\n",
      "    arr[i] = allowed_values[i]\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_collections.py\", line 228, in __setitem__\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_collections.py\", line 211, in __set_item\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 332, in get_return_value\n",
      "py4j.protocol.Py4JError: An error occurred while calling None.None. Trace:\n",
      "py4j.Py4JException: Cannot convert java.lang.Integer to java.lang.String\n",
      "#011at py4j.commands.ArrayCommand.convertArgument(ArrayCommand.java:160)\n",
      "#011at py4j.commands.ArrayCommand.setArray(ArrayCommand.java:144)\n",
      "#011at py4j.commands.ArrayCommand.execute(ArrayCommand.java:97)\n",
      "#011at py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "#011at java.lang.Thread.run(Thread.java:748)\n",
      "25/12/01 09:20:43 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.73.44:46687 in memory (size: 3.3 KB, free: 1008.8 MB)\n",
      "25/12/01 09:20:43 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:33591 in memory (size: 3.3 KB, free: 13.8 GB)\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 134\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 110\n",
      "25/12/01 09:20:43 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.73.44:46687 in memory (size: 60.3 KB, free: 1008.9 MB)\n",
      "25/12/01 09:20:43 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:33591 in memory (size: 60.3 KB, free: 13.8 GB)\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 148\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 119\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 159\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 126\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 157\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 127\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 118\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 111\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 167\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 125\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 130\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 113\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 123\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 117\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 140\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 162\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 142\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 136\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 146\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 116\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 165\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 115\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 166\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 154\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 131\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 135\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 168\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 161\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 164\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 151\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 137\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 132\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 128\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 121\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 129\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 147\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 152\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 114\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 141\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 122\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 143\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 120\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 149\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 160\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 124\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 153\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 155\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned shuffle 1\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 144\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 138\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 133\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 158\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 112\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 139\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 145\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 163\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 156\n",
      "25/12/01 09:20:43 INFO spark.ContextCleaner: Cleaned accumulator 150\n",
      "25/12/01 09:23:09 INFO BlockStateChange: BLOCK* processReport 0xb9fc206ac781ace6: from storage DS-aed0ef10-7760-4811-929e-73dcaca64999 node DatanodeRegistration(10.0.73.44:50010, datanodeUuid=1e5a010a-d1e6-4909-aeba-0576cfa57c89, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-962a1ce9-863c-4293-adfe-92a719625e96;nsid=465826344;c=1764580798938), blocks: 8, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0\n",
      "25/12/01 09:23:09 INFO datanode.DataNode: Successfully sent block report 0xb9fc206ac781ace6,  containing 1 storage report(s), of which we sent 1. The reports had 8 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\n",
      "25/12/01 09:23:09 INFO datanode.DataNode: Got finalize command for block pool BP-1522574362-10.0.73.44-1764580798938\n",
      "25/12/01 09:30:00 INFO scheduler.AbstractYarnScheduler: Release request cache is cleaned up\n",
      "25/12/01 09:30:01 INFO localizer.ResourceLocalizationService: Cache Size Before Clean: 418658024, Total Deleted: 0, Public Deleted: 0, Private Deleted: 0\n",
      "25/12/01 09:34:18 INFO datanode.DirectoryScanner: BlockPool BP-1522574362-10.0.73.44-1764580798938 Total blocks: 8, missing metadata files:0, missing block files:0, missing blocks in memory:0, mismatched blocks:0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Job ended with status 'Stopped' rather than 'Completed'. This could mean the job timed out or stopped early for some other reason: Consider checking whether it completed as you expect.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Define PySparkProcessor for Deequ job ---\n",
    "processor = PySparkProcessor(\n",
    "    base_job_name=\"spark-retail-demand-analyzer\",  # prefix ของชื่อ ProcessingJob\n",
    "    role=role,\n",
    "    framework_version=\"2.4\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    "    max_runtime_in_seconds=900,  # 15 นาที\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "# --- Run Spark + Deequ processing job ---\n",
    "# ต้องมี:\n",
    "#   - preprocess-deequ-pyspark.py   (สคริปต์ที่เรียก pydeequ)\n",
    "#   - deequ-1.0.3-rc2.jar          (JAR ของ Deequ)\n",
    "processor.run(\n",
    "    submit_app=\"preprocess-deequ-pyspark.py\",\n",
    "    submit_jars=[\"deequ-1.0.3-rc2.jar\"],\n",
    "    arguments=[\n",
    "        \"s3_input_data\", s3_input_data,\n",
    "        \"s3_output_analyze_data\", s3_output_analyze_data,\n",
    "    ],\n",
    "    logs=True,   # แสดง log ของ Spark/Deequ ใน notebook\n",
    "    wait=True    # รอจน job จบ\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-423623839320/retail-demand-deequ-2025-12-01-09-17-02/output/dataset-metrics/_SUCCESS to generated_deequ_report/dataset-metrics/_SUCCESS\n",
      "download: s3://sagemaker-us-east-1-423623839320/retail-demand-deequ-2025-12-01-09-17-02/output/dataset-metrics/part-00000-6b7c4ef0-8678-4254-828f-361a8535d4c8-c000.csv to generated_deequ_report/dataset-metrics/part-00000-6b7c4ef0-8678-4254-828f-361a8535d4c8-c000.csv\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p generated_deequ_report\n",
    "!aws s3 cp --recursive $s3_output_analyze_data ./generated_deequ_report/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE output/\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls  $s3_output_analyze_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Deequ Spark Processing job completed.\n",
      "🔎 Deequ result root S3 prefix: s3://sagemaker-us-east-1-423623839320/retail-demand-deequ-2025-12-01-09-17-02/output\n",
      "Stored 's3_output_analyze_data' (str)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n✅ Deequ Spark Processing job completed.\")\n",
    "print(\"🔎 Deequ result root S3 prefix:\", s3_output_analyze_data)\n",
    "\n",
    "# เก็บ path นี้ไว้ใช้โหลดผลลัพธ์ใน notebook อื่น/ภายหลัง\n",
    "%store s3_output_analyze_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science_on_aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
